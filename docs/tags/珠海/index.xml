<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>珠海 on Tan Jay | 唐 洁</title>
    <link>//localhost:1313/tags/%E7%8F%A0%E6%B5%B7/</link>
    <description>Recent content in 珠海 on Tan Jay | 唐 洁</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Feb 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1313/tags/%E7%8F%A0%E6%B5%B7/" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>DRO研究梳理</title>
      <link>//localhost:1313/cn/2025/02/27/dro/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2025/02/27/dro/</guid>
      <description>
        <![CDATA[
        <blockquote>
<p>Distribution Shift 与 Distribution Robust Optimization 之间的关系为：问题与方法。</p>
</blockquote>
<hr>
<h2 id="现实问题">现实问题</h2>
<p>在机器学习中，<strong>Distribution Shift（分布偏移）</strong> 是指模型在训练阶段使用的数据分布与测试阶段（或实际部署时）的数据分布不一致的现象。这种不一致性可能导致<strong>模型在实际应用中出现性能下降</strong>，因为模型假设训练数据和测试数据来自同一分布的前提被打破。</p>
<h3 id="distribution-shift-的常见类型">Distribution Shift 的常见类型</h3>
<ol>
<li>
<p><strong>协变量偏移（Covariate Shift）</strong></p>
<ul>
<li><strong>问题</strong>：输入特征（X）的分布发生变化，但标签条件分布（P(Y|X)）保持不变。</li>
<li><strong>例子</strong>：训练数据是白天的街景图片，而测试数据是夜间图片。</li>
</ul>
</li>
<li>
<p><strong>标签偏移（Label Shift）</strong></p>
<ul>
<li><strong>问题</strong>：标签（Y）的分布发生变化，但特征条件分布（P(X|Y)）保持不变。</li>
<li><strong>例子</strong>：训练时疾病诊断数据中健康样本占多数，但测试时患病样本占多数。</li>
</ul>
</li>
<li>
<p><strong>概念偏移（Concept Shift）</strong></p>
<ul>
<li><strong>问题</strong>：输入特征和标签的映射关系（P(Y|X)）发生变化。</li>
<li><strong>例子</strong>：用户对“好电影”的定义随时间变化（如评分标准改变）。</li>
</ul>
</li>
<li>
<p><strong>系统性偏移（Systematic Shift）</strong></p>
<ul>
<li><strong>问题</strong>：数据生成机制发生变化。</li>
<li><strong>例子</strong>：传感器校准改变或采集环境变化。</li>
</ul>
</li>
</ol>
<h2 id="解决方案">解决方案</h2>
<p>针对 Distribution Shift 问题，Distribution Robust Optimization 是对应的解决方案：一种优化框架，旨在直接建模分布的不确定性，通过最坏情况（Worst-Case）优化，使模型对潜在的分布偏移具有鲁棒性。二者关系更详细的阐述可见<a href="/cn/2025/02/27/shift/">此处</a>。</p>
<h3 id="dro-的数学目标"><strong>DRO 的数学目标</strong></h3>
<p>DRO 的优化目标不是最小化训练数据分布（即经验分布）上的风险，而是最小化<strong>某个不确定性集合（Uncertainty Set）内所有可能分布的最大风险</strong>：
<code>$ \min_{\theta} \max_{Q \in \mathcal{Q}} \mathbb{E}_{(x,y) \sim Q} [\mathcal{L}(f_\theta(x), y)] $</code></p>
<p>其中：</p>
<ul>
<li><code>$\mathcal{Q}$</code> 是围绕训练数据分布 <code>$P_{\text{train}}$</code> 构建的分布集合（如 Wasserstein 球内的分布）。</li>
<li><strong>目标</strong>：通过优化最坏情况（<code>$\max_{Q}$</code>）的损失，确保模型在分布偏移时依然稳定。</li>
</ul>
<h3 id="挑战与难题"><strong>挑战与难题</strong></h3>
<ol>
<li>
<p><strong>不确定性集合的设计</strong>：如何合理定义分布集合 <code>$\mathcal{Q}$</code>（如选择距离度量、半径大小）。若 <code>$\mathcal{Q}$</code> 过小，无法覆盖实际偏移；若过大，模型可能过于保守，导致性能下降；若测试分布完全超出 <code>$\mathcal{Q}$</code> 的覆盖范围（如从自然图像转移到抽象艺术），DRO 的鲁棒性保证失效。</p>
</li>
<li>
<p><strong>计算复杂性</strong>：DRO 需要求解内层的 <code>$\max_{Q}$</code> 优化问题，可能带来较高的计算成本（尤其对高维数据）。</p>
</li>
</ol>
<h2 id="历史已有研究方法">历史已有研究方法</h2>
<h4 id="纵向回顾-时间线">纵向回顾（时间线）</h4>
<h3 id="应对-distribution-shift-的研究方法"><strong>应对 Distribution Shift 的研究方法</strong></h3>
<h4 id="1-检测与诊断">1. <strong>检测与诊断</strong> （✅）</h4>
<ul>
<li><strong>统计检验</strong>：使用 KL 散度、最大均值差异（MMD）或假设检验（如卡方检验）量化分布差异。</li>
<li><strong>模型性能监控</strong>：实时监测模型在测试环境中的性能下降，触发重新训练或报警。</li>
</ul>
<h4 id="2-分布适应-domain-adaptation">2. <strong>分布适应（Domain Adaptation）</strong></h4>
<ul>
<li><strong>无监督域适应（Unsupervised DA）</strong>：在目标域无标签的情况下，对齐源域和目标域的特征分布（如通过对抗训练、域混淆损失）。</li>
<li><strong>重要性加权（Importance Weighting）</strong>：对训练样本加权，使源域数据在目标域分布下重新加权（如通过密度比率估计）。</li>
<li><strong>特征对齐</strong>：学习域不变特征表示（如使用对抗网络或领域特定归一化）。</li>
</ul>
<h4 id="3-鲁棒性增强">3. <strong>鲁棒性增强</strong></h4>
<ul>
<li><strong>数据增强</strong>：通过合成多样化数据（如对抗样本生成、风格迁移）覆盖潜在的分布变化。</li>
<li><strong>对抗训练</strong>：在训练中引入扰动，增强模型对输入变化的鲁棒性。</li>
<li><strong>不变性学习</strong>：强制模型学习与分布无关的特征（如因果推断中的不变性假设）。</li>
</ul>
<h4 id="4-动态适应与在线学习">4. <strong>动态适应与在线学习</strong></h4>
<ul>
<li><strong>持续学习（Continual Learning）</strong>：在部署过程中持续更新模型，适应新分布。</li>
<li><strong>元学习（Meta-Learning）</strong>：学习一个能快速适应新分布的初始化模型（如 MAML）。</li>
</ul>
<h4 id="5-不确定性估计">5. <strong>不确定性估计</strong> （✅）</h4>
<ul>
<li><strong>校准置信度</strong>：使用温度缩放（Temperature Scaling）或贝叶斯方法校准模型预测的不确定性。</li>
<li><strong>拒绝机制</strong>：对低置信度的样本拒绝预测，避免分布外的错误。</li>
</ul>
<h4 id="6-因果推断方法">6. <strong>因果推断方法</strong></h4>
<ul>
<li><strong>因果特征学习</strong>：基于因果图识别稳定特征（如干预不变性），减少对虚假相关性的依赖。</li>
<li><strong>反事实增强</strong>：生成反事实样本，增强模型对分布变化的泛化能力。</li>
</ul>
<h4 id="7-半监督与自监督学习">7. <strong>半监督与自监督学习</strong></h4>
<ul>
<li><strong>利用目标域未标注数据</strong>：通过自监督预训练（如对比学习）提取目标域特征。</li>
<li><strong>伪标签（Pseudo-Labeling）</strong>：用模型对目标域数据生成伪标签进行微调。</li>
</ul>
<h3 id="横向回顾-个人线">横向回顾（个人线）</h3>
<h4 id="duchi">Duchi</h4>
<h4 id="candes">Candes</h4>
<h2 id="未来可行研究方向">未来可行研究方向</h2>
<ul>
<li><strong>数据高效适应</strong>：如何在目标域标注数据极少时有效适应。</li>
<li><strong>实时性要求</strong>：动态环境（如自动驾驶）中模型的快速适应能力。</li>
<li><strong>可解释性</strong>：诊断分布偏移的具体原因（如特征级偏移 vs. 标签级偏移）。</li>
<li><strong>多源迁移</strong>：从多个源域迁移知识到目标域。</li>
</ul>
<p>Distribution Shift 是实际部署中的核心挑战之一，研究需结合具体场景（如医疗、金融、自动驾驶）的特点，设计针对性的解决方案。</p>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>不确定量化</title>
      <link>//localhost:1313/cn/2025/02/27/uc/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2025/02/27/uc/</guid>
      <description>
        <![CDATA[
        <h1 id="不确定量化">不确定量化</h1>
<h2 id="与分布偏移的关系">与分布偏移的关系</h2>
<p>不确定量化（Uncertainty Quantification）与分布偏移（Distribution Shift）在机器学习中是紧密相关的两个问题。它们的关系可以从以下几个方面理解：</p>
<hr>
<h3 id="1-分布偏移会引发模型的不确定性变化"><strong>1. 分布偏移会引发模型的不确定性变化</strong></h3>
<ul>
<li><strong>现象</strong>：当模型遇到分布外的数据（即测试数据分布与训练数据不同）时，其预测的<strong>不确定性（如置信度、方差）通常会显著增加</strong>，甚至可能给出错误的“过度自信”预测。</li>
<li><strong>意义</strong>：
<ul>
<li>不确定性可以作为<strong>分布偏移的检测信号</strong>。例如，模型对某些样本的预测不确定性突然升高，可能暗示这些样本来自新分布（即发生了分布偏移）。</li>
<li>在分布偏移场景中，<strong>校准不确定性</strong>（即确保模型输出的置信度与真实准确率匹配）尤为重要。未校准的不确定性会导致误导性决策（如自动驾驶中对陌生场景的过度自信）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-不确定性量化是应对分布偏移的关键工具"><strong>2. 不确定性量化是应对分布偏移的关键工具</strong></h3>
<h4 id="1-检测分布偏移"><strong>(1) 检测分布偏移</strong></h4>
<ul>
<li><strong>方法</strong>：通过模型输出的不确定性（如预测熵、方差、置信度）判断样本是否来自新分布。
<ul>
<li>例如：使用<strong>离群检测</strong>（Out-of-Distribution Detection），若模型对某样本的预测熵远高于训练数据，则可能属于分布外样本。</li>
<li>工具：蒙特卡洛 Dropout（MC Dropout）、深度集成（Deep Ensemble）等方法可估计模型的不确定性。</li>
</ul>
</li>
</ul>
<h4 id="2-自适应决策"><strong>(2) 自适应决策</strong></h4>
<ul>
<li><strong>拒绝机制</strong>：当模型对某样本的不确定性超过阈值时，拒绝预测并交由人工处理（如医疗诊断中的高风险案例）。</li>
<li><strong>动态资源分配</strong>：在边缘计算中，对高不确定性样本分配更多计算资源（如调用更复杂的模型）。</li>
</ul>
<h4 id="3-鲁棒模型训练"><strong>(3) 鲁棒模型训练</strong></h4>
<ul>
<li><strong>对抗训练</strong>：通过最大化模型在扰动样本上的不确定性，增强对分布偏移的鲁棒性。</li>
<li><strong>不确定性正则化</strong>：在损失函数中加入不确定性惩罚项，防止模型对训练分布过拟合。</li>
</ul>
<hr>
<h3 id="3-分布偏移挑战不确定性量化的可靠性"><strong>3. 分布偏移挑战不确定性量化的可靠性</strong></h3>
<ul>
<li><strong>问题</strong>：传统不确定性估计方法（如贝叶斯神经网络）通常假设训练和测试数据同分布，但在分布偏移下可能失效。
<ul>
<li>例如：模型可能对分布外样本给出<strong>错误的高置信度</strong>（即“自信的错误”）。</li>
</ul>
</li>
<li><strong>解决方法</strong>：
<ul>
<li><strong>领域自适应不确定性校准</strong>：在目标域数据（即使无标签）上重新校准模型的不确定性（如使用温度缩放）。</li>
<li><strong>因果不确定性建模</strong>：分离不确定性中与分布偏移无关的因果因素（如物体形状）和相关的非因果因素（如背景颜色）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-实际应用中的协同作用"><strong>4. 实际应用中的协同作用</strong></h3>
<h4 id="案例1-自动驾驶"><strong>案例1：自动驾驶</strong></h4>
<ul>
<li><strong>分布偏移</strong>：训练数据多为晴天场景，测试时遇到暴雨。</li>
<li><strong>不确定性量化</strong>：模型对雨天图像预测的不确定性升高，触发安全模式（如降速或提醒驾驶员接管）。</li>
</ul>
<h4 id="案例2-医疗诊断"><strong>案例2：医疗诊断</strong></h4>
<ul>
<li><strong>分布偏移</strong>：训练数据来自年轻患者，测试数据为老年患者。</li>
<li><strong>不确定性量化</strong>：模型对老年患者的预测给出高不确定性，提示医生需结合其他检查。</li>
</ul>
<hr>
<h3 id="5-研究方法与前沿方向"><strong>5. 研究方法与前沿方向</strong></h3>
<ul>
<li><strong>不确定性校准 + 分布适应</strong><br>
结合领域自适应（Domain Adaptation）技术，在适应目标域分布的同时校准不确定性（如 <a href="https://arxiv.org/abs/2207.01587">CAN</a> 方法）。</li>
<li><strong>贝叶斯方法与分布鲁棒优化</strong>（✅）<br>
使用贝叶斯神经网络或分布鲁棒优化（DRO）直接建模分布偏移下的不确定性。</li>
<li><strong>因果不确定性分解</strong><br>
从因果视角区分不确定性的来源（如 <a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a>）。</li>
</ul>
<hr>
<h3 id="总结关系"><strong>总结关系</strong></h3>
<table>
<thead>
<tr>
<th><strong>不确定性量化</strong></th>
<th><strong>分布偏移</strong></th>
<th><strong>交互作用</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>检测信号</td>
<td>诱因</td>
<td>通过不确定性升高<strong>预警分布偏移</strong></td>
</tr>
<tr>
<td>决策依据</td>
<td>应对手段</td>
<td>基于不确定性<strong>动态调整</strong>模型行为</td>
</tr>
<tr>
<td>校准目标</td>
<td>挑战</td>
<td>分布偏移可能导致未校准的不确定性</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="关键要点"><strong>关键要点</strong></h3>
<ul>
<li>不确定性量化是<strong>检测和缓解分布偏移的核心工具</strong>。</li>
<li>分布偏移会破坏传统不确定性估计的可靠性，需设计<strong>领域自适应的不确定性方法</strong>。</li>
<li>两者结合可提升模型在开放环境中的<strong>安全性与可信度</strong>（如医疗、自动驾驶等高风险场景）。</li>
</ul>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>分布偏移</title>
      <link>//localhost:1313/cn/2025/02/27/shift/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2025/02/27/shift/</guid>
      <description>
        <![CDATA[
        <h2 id="与dro关系">与DRO关系</h2>
<p>在机器学习中，<strong>Distribution Robust Optimization（分布鲁棒优化，DRO）</strong> 和 <strong>Distribution Shift（分布偏移）</strong> 是紧密相关的两个概念，二者的关系可以从以下角度理解：</p>
<h3 id="1-核心关系"><strong>1. 核心关系</strong></h3>
<ul>
<li><strong>Distribution Shift</strong> 是 <strong>问题</strong>：描述模型在训练和部署时面临的数据分布不一致现象（如协变量偏移、标签偏移等），导致模型性能下降。</li>
<li><strong>Distribution Robust Optimization</strong> 是 <strong>解决方案</strong>：一种优化框架，旨在直接建模分布的不确定性，通过<strong>最坏情况（Worst-Case）优化</strong>，使模型对潜在的分布偏移具有鲁棒性。</li>
</ul>
<p>简言之，<strong>DRO 是应对 Distribution Shift 的一种主动防御方法</strong>，而 Distribution Shift 是 DRO 需要解决的核心挑战。</p>
<h3 id="2-具体关联"><strong>2. 具体关联</strong></h3>
<h4 id="1-dro-的数学目标"><strong>(1) DRO 的数学目标</strong></h4>
<p>DRO 的优化目标不是最小化训练数据分布（即经验分布）上的风险，而是最小化<strong>某个不确定性集合（Uncertainty Set）内所有可能分布的最大风险</strong>：
<code>$ \min_{\theta} \max_{Q \in \mathcal{Q}} \mathbb{E}_{(x,y) \sim Q} [\mathcal{L}(f_\theta(x), y)] $</code></p>
<p>其中：</p>
<ul>
<li><code>$\mathcal{Q}$</code> 是围绕训练数据分布 <code>$P_{\text{train}}$</code> 构建的分布集合（如 Wasserstein 球内的分布）。</li>
<li><strong>目标</strong>：通过优化最坏情况（<code>$\max_{Q}$</code>）的损失，确保模型在分布偏移时依然稳定。</li>
</ul>
<h4 id="2-与-distribution-shift-的联系"><strong>(2) 与 Distribution Shift 的联系</strong></h4>
<ul>
<li>如果测试分布 <code>$P_{\text{test}}$</code> 属于 DRO 定义的集合 <code>$\mathcal{Q}$</code>，则 DRO 训练的模型在 <code>$P_{\text{test}}$</code> 上的性能有理论保障。</li>
<li><strong>DRO 的关键假设</strong>：测试分布 <code>$P_{\text{test}}$</code> 与训练分布 <code>$P_{\text{train}}$</code> 的差异不超过某个范围（由 <code>$\mathcal{Q}$</code> 的半径控制）。
<ul>
<li>若实际分布偏移超出 <code>$\mathcal{Q}$</code> 的范围，DRO 的鲁棒性可能失效。</li>
</ul>
</li>
</ul>
<h3 id="3-dro-如何应对-distribution-shift"><strong>3. DRO 如何应对 Distribution Shift</strong></h3>
<h4 id="1-对协变量偏移-covariate-shift-的鲁棒性"><strong>(1) 对协变量偏移（Covariate Shift）的鲁棒性</strong></h4>
<ul>
<li>DRO 通过约束输入特征 <code>$x$</code> 的分布变化范围（如 Wasserstein 距离约束），直接覆盖协变量偏移场景。</li>
<li><strong>例子</strong>：训练数据是晴天图片，测试数据是雨天图片。若雨天分布在 <code>$\mathcal{Q}$</code> 内，DRO 模型仍能保持性能。</li>
</ul>
<h4 id="2-对标签偏移-label-shift-的鲁棒性"><strong>(2) 对标签偏移（Label Shift）的鲁棒性</strong></h4>
<ul>
<li>若 <code>$\mathcal{Q}$</code> 包含标签分布 <code>$P(y)$</code> 的变化，DRO 可缓解标签偏移的影响（如医疗诊断中患病率变化）。</li>
</ul>
<h4 id="3-对未知偏移类型的保守防御"><strong>(3) 对未知偏移类型的保守防御</strong></h4>
<ul>
<li>DRO 不假设具体的偏移类型（如协变量或标签偏移），而是通过最坏情况优化提供一种<strong>保守但通用的鲁棒性</strong>。</li>
</ul>
<h3 id="4-局限性"><strong>4. 局限性</strong></h3>
<h4 id="1-计算复杂性"><strong>(1) 计算复杂性</strong></h4>
<ul>
<li>DRO 需要求解内层的 <code>$\max_{Q}$</code> 优化问题，可能带来较高的计算成本（尤其对高维数据）。</li>
</ul>
<h4 id="2-不确定性集合的设计"><strong>(2) 不确定性集合的设计</strong></h4>
<ul>
<li><strong>关键挑战</strong>：如何合理定义分布集合 <code>$\mathcal{Q}$</code>（如选择距离度量、半径大小）。
<ul>
<li>若 <code>$\mathcal{Q}$</code> 过小，无法覆盖实际偏移；若过大，模型可能过于保守，导致性能下降。</li>
</ul>
</li>
</ul>
<h4 id="3-对极端偏移的脆弱性"><strong>(3) 对极端偏移的脆弱性</strong></h4>
<ul>
<li>若测试分布完全超出 <code>$\mathcal{Q}$</code> 的覆盖范围（如从自然图像转移到抽象艺术），DRO 的鲁棒性保证失效。</li>
</ul>
<h3 id="5-实际应用场景"><strong>5. 实际应用场景</strong></h3>
<h4 id="1-高风险领域"><strong>(1) 高风险领域</strong></h4>
<ul>
<li><strong>金融风控</strong>：贷款申请数据分布随时间变化（如经济危机），DRO 可减少模型失效风险。</li>
<li><strong>医疗诊断</strong>：患者群体分布变化（如新人群、新设备），DRO 提供稳定性保障。</li>
</ul>
<h4 id="2-数据稀缺场景"><strong>(2) 数据稀缺场景</strong></h4>
<ul>
<li>当目标域数据难以获取时，DRO 通过理论驱动的分布覆盖，替代传统领域自适应（Domain Adaptation）的数据依赖方法。</li>
</ul>
<h3 id="6-与其他方法的对比"><strong>6. 与其他方法的对比</strong></h3>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>核心思想</strong></th>
<th><strong>与 Distribution Shift 的关系</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>经验风险最小化 (ERM)</strong></td>
<td>最小化训练数据上的平均损失</td>
<td>忽略分布偏移，在偏移下性能可能崩溃</td>
</tr>
<tr>
<td><strong>领域自适应 (DA)</strong></td>
<td>对齐源域和目标域的特征分布</td>
<td>依赖目标域数据，需明确知道偏移存在</td>
</tr>
<tr>
<td><strong>分布鲁棒优化 (DRO)</strong></td>
<td>最小化最坏情况分布下的损失</td>
<td>不依赖目标域数据，主动防御潜在偏移</td>
</tr>
</tbody>
</table>
<h3 id="7-前沿研究方向"><strong>7. 前沿研究方向</strong></h3>
<ol>
<li><strong>高效不确定性集合设计</strong>
<ul>
<li>如何结合先验知识（如物理规律、因果结构）构建更合理的 <code>$\mathcal{Q}$</code>。</li>
</ul>
</li>
<li><strong>动态 DRO</strong>
<ul>
<li>在在线学习或持续学习中，动态调整 <code>$\mathcal{Q}$</code> 以适应实时分布变化。</li>
</ul>
</li>
<li><strong>DRO 与因果推断结合</strong>
<ul>
<li>通过因果图识别分布偏移的稳定特征（如 <a href="https://arxiv.org/abs/1907.02893">Invariant Risk Minimization</a>），优化对不变量的鲁棒性。</li>
</ul>
</li>
<li><strong>可扩展优化算法</strong>
<ul>
<li>开发更高效的优化方法（如对偶化、随机梯度下降），降低 DRO 的计算成本。</li>
</ul>
</li>
</ol>
<h3 id="总结"><strong>总结</strong></h3>
<p>DRO 和 Distribution Shift 二者关系可类比为 <strong>“防御（DRO）” vs “攻击（Distribution Shift）”</strong>，DRO 为模型穿上了一层针对分布偏移的“盔甲”。</p>
<ul>
<li><strong>DRO 是应对 Distribution Shift 的数学框架</strong>：通过最坏情况优化，主动增强模型对分布变化的鲁棒性。</li>
<li><strong>适用条件</strong>：测试分布需在预设的不确定性集合 <code>$\mathcal{Q}$</code> 内，且 <code>$\mathcal{Q}$</code> 的设计需要领域知识。</li>
<li><strong>优势</strong>：不依赖目标域数据，适合数据稀缺或偏移类型未知的场景。</li>
<li><strong>挑战</strong>：平衡鲁棒性与泛化性，避免过度保守。</li>
</ul>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>研究主题</title>
      <link>//localhost:1313/cn/2025/02/26/topics/</link>
      <pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2025/02/26/topics/</guid>
      <description>
        <![CDATA[
        <h1 id="distributional-robust-optimization-dro-cn-2025-02-27-dro">distributional robust optimization (<a href="/cn/2025/02/27/dro/">DRO</a>)</h1>
<p>经验似然，保形推断，公平性，差分隐私，<a href="/cn/2025/02/27/uc/">不确定量化</a>，f散度与DRO关系紧密，其中公平性，差分隐私是distribution shift 包装问题；经验似然，保形推断是在 distribution shift 问题背景下保持分布稳健的方法；f散度度量了 distribution shift 的偏移程度；不确定量化是检测和缓解 distribution shift 的核心工具。这些方法的结合可提升模型在开放环境中的<strong>安全性与可信度</strong>（如医疗、自动驾驶等高风险场景）。</p>
<h2 id="经验似然-empirical-likelihood">经验似然 empirical likelihood</h2>
<h2 id="保形推断-conformal-inference-cn-2025-02-25-conformity">保形推断 <a href="/cn/2025/02/25/conformity/">conformal inference</a></h2>
<h2 id="公平性-fairness">公平性 fairness</h2>
<ul>
<li>文章</li>
<li>代码
<ul>
<li><a href="https://fate-computing.mpi-sws.org/">https://fate-computing.mpi-sws.org/</a></li>
<li><a href="https://github.com/mbilalzafar/fair-classification">https://github.com/mbilalzafar/fair-classification</a></li>
</ul>
</li>
</ul>
<h2 id="不确定量化-uncertainty-quantification-cn-2025-02-27-uc">不确定量化 <a href="/cn/2025/02/27/uc/">uncertainty quantification</a></h2>
<h2 id="差分隐私-differential-privacy">差分隐私 Differential privacy</h2>
<h2 id="f散度">f散度</h2>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>保形推断</title>
      <link>//localhost:1313/cn/2025/02/25/conformity/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2025/02/25/conformity/</guid>
      <description>
        <![CDATA[
        <h1 id="研究记录之保形推断">研究记录之保形推断</h1>
<p>主题：confermal inference、参考文献、nonconformity measure、重对数律。</p>
<h2 id="confermal-inference">confermal inference</h2>
<p>Conformal Inference（保形推断）是一种非参数的统计方法，用于为预测模型生成具有严格概率保证的预测区间或集合。其核心目标是在不依赖数据分布假设的情况下，确保新观测值的真实结果以预定概率（如95%）落入预测范围内。以下是其关键要点：</p>
<h3 id="核心思想">核心思想</h3>
<ol>
<li><strong>覆盖概率保证</strong>：无论数据分布如何，Conformal Inference生成的预测区间能以指定的置信水平（如1-α）覆盖真实值，适用于有限样本且无需渐近近似。</li>
<li><strong>非参数与模型无关</strong>：不假设数据分布或模型结构，适用于任何预测模型（如线性回归、神经网络等），尤其适合复杂机器学习模型的不确定性量化。</li>
</ol>
<h3 id="关键步骤">关键步骤</h3>
<ol>
<li><strong>划分数据</strong>：将数据集分为训练集和校准集。</li>
<li><strong>训练模型</strong>：使用训练集训练模型。</li>
<li><strong>计算非合群分数（Nonconformity Score）</strong>：衡量预测与实际值的差异。例如：
<ul>
<li><strong>回归任务</strong>：残差绝对值 <code>$ |y_i - \hat{y}_i| $</code> 。</li>
<li><strong>分类任务</strong>：1减去正确类别的预测概率 <code>$ 1 - P(y_i|x_i) $</code>。</li>
</ul>
</li>
<li><strong>确定分位数</strong>：基于校准集的分数计算调整后的分位数 <code>$q = \lceil (n+1)(1-\alpha) \rceil / n $</code> ，其中 <code>$ n $</code>为校准集大小。</li>
<li><strong>构建预测区间</strong>：新样本的预测区间为 <code>$\hat{y}_{\text{new}} \pm q $</code> （回归）或包含概率高于阈值的类别集合（分类）。</li>
</ol>
<h3 id="优势">优势</h3>
<ul>
<li><strong>强理论保证</strong>：严格覆盖概率，无需分布假设。</li>
<li><strong>灵活性</strong>：兼容任何模型，适应回归与分类任务。</li>
<li><strong>实用性强</strong>：适用于小样本，直接反映模型预测的不确定性。</li>
</ul>
<h3 id="局限性">局限性</h3>
<ul>
<li><strong>数据交换性假设</strong>：要求数据满足交换性（弱于独立同分布），可能不适用于时间序列等有序数据。</li>
<li><strong>区间宽度依赖模型质量</strong>：模型预测越准，区间越窄；反之则越宽。</li>
</ul>
<h3 id="应用场景">应用场景</h3>
<ul>
<li><strong>高风险领域</strong>：如医疗诊断（预测疾病风险区间）、金融（风险估值）等需可靠不确定性的场景。</li>
<li><strong>模型评估</strong>：对比不同模型的不确定性估计能力。</li>
</ul>
<h3 id="示例">示例</h3>
<p><strong>回归任务</strong>：校准集残差为[0.5, 1.2, 2.0]，置信水平95%时，调整后分位数取第3大值（2.0）。新预测值为10，则区间为[8.0, 12.0]，保证真实值有95%概率落入。</p>
<p><strong>分类任务</strong>：某样本正确类别的预测概率为0.6，阈值为0.3（对应1-α=95%），则预测集合包含所有概率≥0.4的类别，确保真实类别被包含的概率≥95%。</p>
<p>总之，Conformal Inference通过数据驱动的方法，为复杂模型提供可靠的不确定性估计，增强其在现实应用中的可信度。</p>
<p>以下是关于 <strong>Conformal Inference（保形推断）</strong> 的经典论文和最新研究推荐，涵盖理论、应用及扩展方向。这些论文适合深入理解其数学基础、算法实现及实际应用场景。</p>
<h2 id="参考文献">参考文献</h2>
<h3 id="1-奠基性论文"><strong>1. 奠基性论文</strong></h3>
<h4 id="1-algorithmic-learning-in-a-random-world-https-link-springer-com-book-10-1007-978-3-031-06649-8"><strong>(1) <a href="https://link.springer.com/book/10.1007/978-3-031-06649-8">Algorithmic Learning in a Random World</a></strong></h4>
<ul>
<li><strong>作者</strong>: Vovk, Gammerman, Shafer (2005)</li>
<li><strong>贡献</strong>: 系统提出保形推断的框架，定义了非合群分数（nonconformity score）和覆盖概率保证的数学证明，是保形推断的理论基石。</li>
</ul>
<h4 id="2-conformal-prediction-for-reliable-machine-learning-https-arxiv-org-abs-1404-1393"><strong>(2) <a href="https://arxiv.org/abs/1404.1393">Conformal Prediction for Reliable Machine Learning</a></strong></h4>
<ul>
<li><strong>作者</strong>: Balasubramanian, Ho, Vovk (2014)</li>
<li><strong>贡献</strong>: 综述性论文，总结保形推断在分类、回归、异常检测等任务中的应用，并讨论与贝叶斯方法的对比。</li>
</ul>
<h3 id="2-分类与回归任务"><strong>2. 分类与回归任务</strong></h3>
<h4 id="3-conformal-prediction-under-covariate-shift-https-arxiv-org-abs-1904-06019"><strong>(3) <a href="https://arxiv.org/abs/1904.06019">Conformal Prediction Under Covariate Shift</a></strong></h4>
<ul>
<li><strong>作者</strong>: Tibshirani et al. (2019)</li>
<li><strong>贡献</strong>: 提出协变量偏移（covariate shift）下的保形推断方法，扩展了传统方法的适用范围。</li>
</ul>
<h4 id="4-distribution-free-predictive-inference-for-regression-https-arxiv-org-abs-1802-06307"><strong>(4) <a href="https://arxiv.org/abs/1802.06307">Distribution-Free Predictive Inference for Regression</a></strong></h4>
<ul>
<li><strong>作者</strong>: Lei et al. (2018)</li>
<li><strong>贡献</strong>: 针对回归任务提出分位数回归与保形推断结合的方法（<strong>Conformalized Quantile Regression, CQR</strong>），生成更紧致的预测区间。</li>
</ul>
<h4 id="5-classification-with-valid-and-adaptive-coverage-https-arxiv-org-abs-2006-02544"><strong>(5) <a href="https://arxiv.org/abs/2006.02544">Classification with Valid and Adaptive Coverage</a></strong></h4>
<ul>
<li><strong>作者</strong>: Angelopoulos et al. (2020)</li>
<li><strong>贡献</strong>: 提出自适应保形分类（Adaptive Conformal Classification），动态调整预测集合大小以提升效率。</li>
</ul>
<h3 id="3-时间序列与非交换数据"><strong>3. 时间序列与非交换数据</strong></h3>
<h4 id="6-conformal-prediction-for-time-series-https-arxiv-org-abs-2205-00127"><strong>(6) <a href="https://arxiv.org/abs/2205.00127">Conformal Prediction for Time Series</a></strong></h4>
<ul>
<li><strong>作者</strong>: Xu &amp; Xie (2022)</li>
<li><strong>贡献</strong>: 解决时间序列数据因违反交换性假设（exchangeability）带来的挑战，提出滑动窗口或分块保形推断方法。</li>
</ul>
<h4 id="7-conformal-pid-control-for-time-series-prediction-https-arxiv-org-abs-2307-16895"><strong>(7) <a href="https://arxiv.org/abs/2307.16895">Conformal PID Control for Time Series Prediction</a></strong></h4>
<ul>
<li><strong>作者</strong>: Lindemann et al. (2023)</li>
<li><strong>贡献</strong>: 将保形推断与PID控制结合，动态调整预测区间宽度，适应非平稳时间序列。</li>
</ul>
<h3 id="4-深度学习与高维数据"><strong>4. 深度学习与高维数据</strong></h3>
<h4 id="8-conformal-prediction-for-deep-classifiers-via-clustering-https-arxiv-org-abs-2107-03363"><strong>(8) <a href="https://arxiv.org/abs/2107.03363">Conformal Prediction for Deep Classifiers via Clustering</a></strong></h4>
<ul>
<li><strong>作者</strong>: Lu et al. (2021)</li>
<li><strong>贡献</strong>: 针对深度神经网络分类任务，提出基于聚类的保形推断方法，降低预测集合的冗余性。</li>
</ul>
<h4 id="9-uncertainty-quantification-with-conformal-prediction-for-deep-learning-https-arxiv-org-abs-2207-12254"><strong>(9) <a href="https://arxiv.org/abs/2207.12254">Uncertainty Quantification with Conformal Prediction for Deep Learning</a></strong></h4>
<ul>
<li><strong>作者</strong>: Angelopoulos et al. (2022)</li>
<li><strong>贡献</strong>: 系统性讨论如何将保形推断与深度学习结合，提供代码库（如 <code>TorchCP</code>）实现。</li>
</ul>
<h3 id="5-最新扩展方向"><strong>5. 最新扩展方向</strong></h3>
<h4 id="10-conformal-risk-control-https-arxiv-org-abs-2208-02814"><strong>(10) <a href="https://arxiv.org/abs/2208.02814">Conformal Risk Control</a></strong></h4>
<ul>
<li><strong>作者</strong>: Angelopoulos et al. (2022)</li>
<li><strong>贡献</strong>: 将保形推断推广到更一般的风险控制框架，适用于多任务学习与复杂损失函数。</li>
</ul>
<h4 id="11-conformal-off-policy-prediction-for-contextual-bandits-https-arxiv-org-abs-2306-04410"><strong>(11) <a href="https://arxiv.org/abs/2306.04410">Conformal Off-Policy Prediction for Contextual Bandits</a></strong></h4>
<ul>
<li><strong>作者</strong>: Bastani et al. (2023)</li>
<li><strong>贡献</strong>: 在强化学习（Contextual Bandits）中应用保形推断，解决策略评估的覆盖性问题。</li>
</ul>
<h3 id="6-实用教程与书籍"><strong>6. 实用教程与书籍</strong></h3>
<ul>
<li><strong>书籍</strong>: <a href="https://arxiv.org/abs/2305.12621"><em>Conformal Prediction: A Unified Review of Theory and New Challenges</em></a> (2023)
<ul>
<li>最新综述，涵盖理论、算法及在因果推断、联邦学习等场景的扩展。</li>
</ul>
</li>
<li><strong>教程代码库</strong>:
<ul>
<li><a href="https://github.com/donlnz/nonconformist">Python库 <code>nonconformist</code></a></li>
<li><a href="https://github.com/ShuoYang-1998/TorchCP">TorchCP (PyTorch实现)</a></li>
</ul>
</li>
</ul>
<h3 id="选择建议"><strong>选择建议</strong></h3>
<ul>
<li><strong>入门</strong>：从奠基性论文（1-2）和教程代码库开始，理解核心思想。</li>
<li><strong>应用场景</strong>：
<ul>
<li>时间序列选（6-7），</li>
<li>深度学习选（8-9），</li>
<li>分类回归优化选（3-5）。</li>
</ul>
</li>
<li><strong>理论扩展</strong>：关注（10-11）的前沿方向。</li>
</ul>
<p>保形推断的核心优势在于其非参数性和严格的覆盖保证，但需注意其<strong>数据交换性假设</strong>是否满足（如时间序列需调整方法）。</p>
<h2 id="nonconformity-measure">nonconformity measure</h2>
<p>“<strong>Nonconformity measure</strong>”（非符合性度量）是统计学习和机器学习中的一个术语，尤其在 <strong>Conformal Prediction</strong>（保形预测）框架中扮演核心角色。它用于量化一个数据点与已有数据分布或模型预测的“不一致程度”，从而评估新样本的异常性或不确定性。</p>
<h3 id="核心概念"><strong>核心概念</strong></h3>
<ol>
<li>
<p><strong>基本定义</strong>：</p>
<ul>
<li><strong>Nonconformity measure</strong> 是一个函数，用于计算某个数据点（或样本）与已有数据/模型的“不匹配程度”。</li>
<li>值越大，表示该数据点越不符合当前模型或数据分布，可能属于异常或需要特别关注。</li>
</ul>
</li>
<li>
<p><strong>在 Conformal Prediction 中的作用</strong>：</p>
<ul>
<li>Conformal Prediction 是一种生成预测集合并提供统计置信度的方法，确保预测结果在指定置信水平下覆盖真实值。</li>
<li>通过 <strong>nonconformity measure</strong>，算法会为每个候选预测结果计算一个“不一致分数”，从而确定哪些预测应被包含在置信区间或预测集合中。</li>
</ul>
</li>
</ol>
<h3 id="应用示例"><strong>应用示例</strong></h3>
<ul>
<li>
<p><strong>分类任务</strong>：
假设一个图像分类模型需要判断一张新图片是否属于“猫”。对于每个可能的类别（猫、狗、鸟等），nonconformity measure 可能基于模型输出的概率，计算该图片与各类别训练数据的差异。若“猫”类别的差异分数最低，则该图片更可能被归为“猫”。</p>
</li>
<li>
<p><strong>回归任务</strong>：
在房价预测中，nonconformity measure 可以是预测房价与实际房价的绝对误差。误差越大，样本的“非符合性”越高。</p>
</li>
</ul>
<h3 id="技术意义"><strong>技术意义</strong></h3>
<ul>
<li><strong>异常检测</strong>：高 nonconformity score 可能标志异常值（outlier）。</li>
<li><strong>不确定性量化</strong>：在 Conformal Prediction 中，通过非符合性分数生成预测区间（例如，“房价在 80% 置信度下位于 [500k, 600k]”）。</li>
<li><strong>模型校准</strong>：帮助评估模型对新数据的泛化能力。</li>
</ul>
<h3 id="与其他概念的区别"><strong>与其他概念的区别</strong></h3>
<ul>
<li><strong>Loss Function（损失函数）</strong>：损失函数用于训练模型，而非符合性度量用于评估模型预测与数据的一致性。</li>
<li><strong>Anomaly Score（异常分数）</strong>：两者类似，但 nonconformity measure 更强调统计框架下的置信度保证。</li>
</ul>
<p>简而言之，<strong>nonconformity measure</strong> 是连接数据、模型与统计置信度的桥梁，尤其在需要可靠不确定性估计的场景（如医疗诊断、金融风险评估）中至关重要。</p>
<p>这段话讨论了保形预测（Conformal Prediction）中 <strong>非对称非符合性度量（Asymmetric Nonconformity Measure）</strong> 的设计及其意义。以下是逐层解析：</p>
<h3 id="度量方式"><strong>度量方式</strong></h3>
<ol>
<li>
<p><strong>对称与非对称的对比</strong></p>
<ul>
<li><strong>对称非符合性度量</strong>（如公式 2.30 或 2.32）：通常使用绝对值（如预测误差的绝对值 <code>$ |y_i - \hat{y}_i| $</code>），表示“偏离程度的量级”，不区分方向（如高估或低估）。</li>
<li><strong>非对称非符合性度量</strong>（如公式 2.33 或 2.34）：允许区分方向（如 <code>$ y_i - \hat{y}_i $</code>或 <code>$ \hat{y}_i - y_i $</code>），可衡量样本对某一特定属性的符合程度（例如“标签是否足够大”或“标签是否足够小”）。</li>
</ul>
</li>
<li>
<p><strong>非对称度量的意义</strong></p>
<ul>
<li><strong>公式 2.33</strong>：<code>$ \alpha_i := y_i - \hat{y}_i $</code>
<ul>
<li><strong>含义</strong>：实际值 <code>$ y_i $</code>比预测值 <code>$ \hat{y}_i $</code>大多少。</li>
<li><strong>用途</strong>：衡量样本 <code>$ z_i $</code>对“标签较大”这一属性的符合程度。例如，若 `$\alpha_i$ 很大，说明真实标签远超预测，可能属于异常（或需特别关注的高值样本）。</li>
</ul>
</li>
<li><strong>公式 2.34</strong>：<code>$ \alpha_i := \hat{y}_i - y_i $</code>
<ul>
<li><strong>含义</strong>：预测值 <code>$ \hat{y}_i $</code>比实际值 <code>$ y_i $</code>大多少。</li>
<li><strong>用途</strong>：衡量样本 <code>$ z_i $</code>对“标签较小”这一属性的符合程度。例如，若 <code>$\alpha_i $</code>很大，说明预测显著高估真实值，可能属于低估异常。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="技术意义-1"><strong>技术意义</strong></h3>
<ol>
<li>
<p><strong>灵活建模单侧关注问题</strong></p>
<ul>
<li>在现实场景中，我们可能只关心某一方向的偏差（例如：
<ul>
<li><strong>金融风控</strong>：更关注损失超过预期的样本（即   <code>$ y_i - \hat{y}_i $</code>为正的情况）。</li>
<li><strong>医疗诊断</strong>：更关注检测结果远低于预期的样本（即 <code>$ \hat{y}_i - y_i $</code>为正的情况）。</li>
</ul>
</li>
<li>非对称度量允许针对特定方向定义“非符合性”，从而生成单侧置信区间或异常检测规则。</li>
</ul>
</li>
<li>
<p><strong>与通用框架的关系</strong></p>
<ul>
<li>非对称度量（如 2.33 和 2.34）是通用非符合性度量（公式 2.31）的特例。</li>
<li><strong>通用框架</strong>（公式 2.31）：允许自定义非符合性函数，只需满足“可比较性”（即不同样本的非符合性分数可排序）。</li>
<li><strong>非对称实现</strong>：通过调整符号（如 <code>$ y_i - \hat{y}_i $</code>或 <code>$ \hat{y}_i - y_i $</code>），将方向信息编码到分数中。</li>
</ul>
</li>
</ol>
<h3 id="示例说明"><strong>示例说明</strong></h3>
<h4 id="场景-房价预测"><strong>场景：房价预测</strong></h4>
<ul>
<li><strong>对称度量</strong>：<code>$\alpha_i = |y_i - \hat{y}_i| $</code>
<ul>
<li>关注预测误差的绝对值，无论实际房价高于或低于预测。</li>
</ul>
</li>
<li><strong>非对称度量</strong>：
<ul>
<li><strong>公式 2.33</strong>：<code>$\alpha_i = y_i - \hat{y}_i $</code>
<ul>
<li>正值越大，说明真实房价远高于预测（可能提示模型低估风险）。</li>
</ul>
</li>
<li><strong>公式 2.34</strong>：<code>$\alpha_i = \hat{y}_i - y_i $</code>
<ul>
<li>正值越大，说明预测远高于真实房价（可能提示模型高估风险）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="应用-生成单侧置信区间"><strong>应用：生成单侧置信区间</strong></h4>
<ul>
<li>若使用 <code>$\alpha_i = y_i - \hat{y}_i $</code> ，则可生成 <strong>上限区间</strong>（如“房价有 95% 概率低于 <code>$ \hat{y}_i + \Delta $</code>”）。</li>
<li>若使用 <code>$\alpha_i = \hat{y}_i - y_i $</code> ，则可生成 <strong>下限区间</strong>（如“房价有 95% 概率高于 <code>$ \hat{y}_i - \Delta $</code>”）。</li>
</ul>
<h3 id="与-p-值的关系"><strong>与 p 值的关系</strong></h3>
<ul>
<li><strong>非对称度量影响 p 值计算</strong>：
<ul>
<li>p 值定义为“集合中非符合性分数大于等于当前样本的比例”。</li>
<li>若使用 <code>$\alpha_i = y_i - \hat{y}_i $</code>，p 值小表示真实值显著高于预测（异常高值）；</li>
<li>若使用 <code>$\alpha_i = \hat{y}_i - y_i $</code> ，p 值小表示真实值显著低于预测（异常低值）。</li>
</ul>
</li>
</ul>
<h3 id="总结"><strong>总结</strong></h3>
<ul>
<li><strong>对称 vs 非对称</strong>：
<ul>
<li>对称度量关注偏差的“量级”，非对称度量关注偏差的“方向”。</li>
</ul>
</li>
<li><strong>实际价值</strong>：
<ul>
<li>允许模型针对业务需求（如风险偏好、单侧异常检测）灵活调整置信区间或异常判定规则。</li>
</ul>
</li>
<li><strong>理论一致性</strong>：
<ul>
<li>非对称度量仍属于保形预测的通用框架，仅通过函数设计引入方向信息。</li>
</ul>
</li>
</ul>
<h2 id="重对数律">重对数律</h2>
<p><strong>重对数律（Law of the Iterated Logarithm, LIL）</strong> 是概率论中描述独立同分布随机变量部分和波动性的精确渐近结果。它刻画了随机波动幅度的上下极限，揭示了大数定律和中心极限定理之间的更深层规律。</p>
<h3 id="核心定义"><strong>核心定义</strong></h3>
<p>设 <code>$X_1, X_2, \dots $</code>是独立同分布（i.i.d.）的随机变量，满足：</p>
<ul>
<li>均值 <code>$\mathbb{E}[X_i] = \mu $</code></li>
<li>方差 <code>$\text{Var}(X_i) = \sigma^2 &lt; \infty $</code></li>
</ul>
<p>定义部分和 <code>$S_n = X_1 + X_2 + \dots + X_n $</code>，则重对数律表明：</p>
<p>$$
\limsup_{n \to \infty} \frac{S_n - n\mu}{\sigma \sqrt{2n \log \log n}} = 1 \quad \text{a.s.}
$$</p>
<p>$$
\liminf_{n \to \infty} \frac{S_n - n\mu}{\sigma \sqrt{2n \log \log n}} = -1 \quad \text{a.s.}
$$</p>
<p>即部分和的偏差被限制在 <code>$\pm \sigma \sqrt{2n \log \log n} $</code>内，且此界限是紧的（几乎必然达到）。</p>
<h3 id="直观解释"><strong>直观解释</strong></h3>
<ol>
<li>
<p><strong>波动范围的精确刻画</strong></p>
<ul>
<li><strong>大数定律</strong>： <code>$S_n / n \to \mu $</code>（均值收敛）。</li>
<li><strong>中心极限定理</strong>：偏差按 <code>$\sqrt{n} $</code>增长，服从正态分布。</li>
<li><strong>重对数律</strong>：进一步给出偏差的极值波动幅度，由 <code>$ \sqrt{n \log \log n} $</code>主导，精确到常数因子 <code>$\sigma \sqrt{2} $</code>。</li>
</ul>
</li>
<li>
<p><strong>“几乎必然”收敛</strong>
波动幅度在无限次观测中会被无限次接近上述上下界，但不会持续超出。</p>
</li>
</ol>
<h3 id="关键意义"><strong>关键意义</strong></h3>
<ol>
<li>
<p><strong>理论深度</strong></p>
<ul>
<li>填补了大数定律（收敛性）与中心极限定理（分布形态）之间的空白，描述了极值波动的渐近行为。</li>
</ul>
</li>
<li>
<p><strong>应用场景</strong></p>
<ul>
<li><strong>随机过程分析</strong>：如布朗运动的路径性质。</li>
<li><strong>统计推断</strong>：评估估计量的收敛速度。</li>
<li><strong>金融数学</strong>：资产价格波动幅度的极端情况建模。</li>
</ul>
</li>
</ol>
<h3 id="示例说明-1"><strong>示例说明</strong></h3>
<p>考虑一个简单对称随机游动（如抛硬币）：</p>
<ul>
<li>每次步长 <code>$X_i $</code>为 +1 或 -1，概率各 0.5。</li>
<li>均值 <code>$\mu = 0 $</code>，方差 <code>$ \sigma^2 = 1 $</code>。</li>
</ul>
<p>根据重对数律，部分和 <code>$S_n $</code>的极值波动满足：
`$$
\limsup_{n \to \infty} \frac{S_n}{\sqrt{2n \log \log n}} = 1 \quad \text{a.s.}
$$</p>
<p>这意味着，当 <code>$ n $</code>极大时，随机游动的路径几乎必然会在 <code>$\pm \sqrt{2n \log \log n} $</code>之间无限次触碰边界，但不会持续超出。</p>
<h3 id="与其他定理的关系"><strong>与其他定理的关系</strong></h3>
<table>
<thead>
<tr>
<th><strong>定理</strong></th>
<th><strong>描述</strong></th>
<th><strong>缩放因子</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>大数定律 (LLN)</td>
<td>均值收敛</td>
<td><code>$ n $</code></td>
</tr>
<tr>
<td>中心极限定理 (CLT)</td>
<td>偏差分布趋近正态</td>
<td><code>$\sqrt{n} $</code></td>
</tr>
<tr>
<td>重对数律 (LIL)</td>
<td>极值波动的上下限</td>
<td><code>$ \sqrt{n \log \log n} $</code></td>
</tr>
</tbody>
</table>
<h3 id="注意事项"><strong>注意事项</strong></h3>
<ul>
<li><strong>独立性假设</strong>：随机变量必须独立同分布。</li>
<li><strong>方差有限性</strong>：若方差无限，结论可能不成立。</li>
<li><strong>多维推广</strong>：存在高维版本，但形式更复杂。</li>
</ul>
<h3 id="总结-1"><strong>总结</strong></h3>
<p>重对数律揭示了随机变量部分和的极值波动被严格约束在 <code>$\pm \sigma \sqrt{2n \log \log n} $</code> 内，是概率论中对随机性本质的深刻刻画，为理解复杂随机现象提供了理论基石。</p>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>周三研究汇报</title>
      <link>//localhost:1313/cn/2025/02/25/report/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2025/02/25/report/</guid>
      <description>
        <![CDATA[
        <h2 id="2025-2-26-第一周">2025-2-26 第一周</h2>
<h3 id="汇报内容">汇报内容：</h3>
<ul>
<li>GSPC 修改：Introduction 后面基本实现修改（加分组描述，步骤间逻辑解释，评估指标解释，分类依据等），主要问题集中在Introduction 怎么改。</li>
<li>DSDE 修改：实验数据还没有，只有小调整，没有大进展。</li>
<li>自己看的论文进度：在了解 <a href="/cn/2025/02/25/conformity/">conformal perdiction</a>一些理论，老师给的 UC 也在看。边看论文也一边在构思，自己置信区间这个事情能为神经网络做些什么，目前还没有具体想法。</li>
</ul>
<h3 id="讨论结果">讨论结果：</h3>
<ul>
<li>围绕 robust 问题，寻找 distribution shift 的设置展开，会有什么挑战</li>
<li>shift 的分类</li>
<li>set-up 设计什么问题，OOD detection，估计问题，检验问题</li>
<li>横向问题：Robust，差分隐私，公平性</li>
<li>纵向问题：DRO</li>
<li>研究问题：问题的设计DRO，设计检验的地方（因果推断）</li>
<li>Online Data</li>
<li>发展脉络
<ul>
<li>往前：之前做了什么</li>
<li>往后：之后做了什么 （paper connection 软件）</li>
</ul>
</li>
</ul>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>每日研究进度条</title>
      <link>//localhost:1313/cn/2025/02/25/record/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2025/02/25/record/</guid>
      <description>
        <![CDATA[
        <h2 id="2025-02-23-周日">2025-02-23 周日</h2>
<ul>
<li>conformal inference
<ul>
<li>书籍看完 2.1 节，下次 2.2 节。</li>
<li>收获：一些精确定义，关于 conformal prediction。</li>
</ul>
</li>
<li>改分组对比学习论文</li>
</ul>
<h2 id="2025-02-25-周二">2025-02-25 周二</h2>
<ul>
<li>conformal inference
<ul>
<li>书籍看完 2.2.4节，下次 2.2.9 节。</li>
<li>收获：一些精确定义，关于 nonconformal measures。</li>
</ul>
</li>
<li>改分组对比学习论文</li>
</ul>
<h2 id="2025-02-26-周三">2025-02-26 周三</h2>
<ul>
<li>conformal inference
<ul>
<li>书籍看完 2.3节，下次 2.4节。</li>
<li>收获：conformal inference 的应用。</li>
</ul>
</li>
<li>面见导师</li>
</ul>
<h2 id="2025-02-27-周四">2025-02-27 周四</h2>
<ul>
<li>时间管理：3+4+（晚上上课）</li>
<li>任务：
<ul>
<li>纵向DRO的工作总结</li>
<li>询问大模型DRO可研究方向 <a href="https://huggingface.co/spaces/DAMO-NLP-SG/CoI_Agent">https://huggingface.co/spaces/DAMO-NLP-SG/CoI_Agent</a></li>
</ul>
</li>
</ul>
<h2 id="2025-02-28-周五">2025-02-28 周五</h2>
<ul>
<li>时间管理：4+3+2</li>
<li>任务：
<ul>
<li>横向Duchi的工作总结</li>
<li></li>
<li></li>
</ul>
</li>
</ul>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>2024最后一天</title>
      <link>//localhost:1313/cn/2024/12/31/last-day-of-2024/</link>
      <pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2024/12/31/last-day-of-2024/</guid>
      <description>
        <![CDATA[
        <p>今天是一年最后一天。
集中在这两三天也发生了很多事情。先说最近一件最乌龙的事吧。
在很早邱师兄说让我代替他的助教去监考，我把事情一直记在备忘录里，到了当天，我也是提前到了，按照培训的要求，自己的手机静音了，并组织学生入座，邱师兄也到了，那是一种心安的感觉。考试进行差不多半个钟，一切都井然有序了，邱师兄也在讲台前坐下来，我在教室的最后一排，悄悄掏出了手机看了一眼，就接到教务彭老师的电话，接到电话的一句话就是“你自己有考试怎么没有去？”我人一下就傻了，我记错自己考试时间了。我匆匆跑向讲台，跟师兄说自己有考试要离开，便匆匆上楼奔向自己的考场。得亏监考在一楼，自己考场在同一栋的四楼。当我到达教室的时候，我感觉到一种所有人松了一口气的感觉，可算是找到我人了。在我坐下不久，我看到邱师兄也来我的考场了，然后把监考的童老师喊出去了，我知道一定是帮我解释来了。这是开卷考试，需要带书，我什么也没带，感谢雷吉安，他说他很快抄完把书借给我；感谢王明涵室友，带了两本书，一本借给了我。考试过程中，因为来得晚，借到书也晚，我只能奋笔疾书了，那个字真的是不忍直视，与我想象中好好考，好好誊写，一点都不一样呀！在准点考试结束的时候，我也尽我所能都写完了，虽然不完美，但用力了。童老师也很惊讶，写完啦？交卷之后，童老师说，“我们已经定了，三个人只能选一个优秀，我们都定好了，唐洁是优秀，但你考试迟到了，现在不一定了。”我赶紧回答说：“能得到老师认可非常开心了，结果不重要，非常感谢老师。”是真的很开心，虽然不会，但认真去弄，尽量给到一个对得起大家花时间来听，这个过程中自己不仅学到了知识，还能得到老师认可，我真的感觉挺幸福的。很感慨，被看见很幸福呀！花出去的时间和精力都值得了，想哭。</p>
<p>然后，七七八八聊聊最近其他事情。我感觉自己的时间被分走了，分哪了？分在做市场调查，分在阅卷，分在志愿者时长，分在听报告，分在准备上台讲课，分在与毕业无关的项目等等。每一件分走我时间的事情我都有不得不做的理由，比如，这是谢导交代的，必须做，邱师兄需要帮忙，无条件做，自己要获得毕业的学分，必须讲课，大家会来听，必须好好准备等。我发现在正式上学期间，七七八八的事情会找上来，也确实不能拒绝，我在学会调整自己的心态，不能抱着以前学生的心态，上学就上工，放假就放松，应该反过来，在放假不被打扰的时间里更加珍惜时间，更集中注意力，更精准用力。</p>
<p>最后，2024 年最后一天，我向神灵许愿，希望明年此时，我能有找到关于神经网络稳健性研究的切入点，并能开始写作，再贪心一点，家人身体健康。🙏🙏</p>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>与炎哥交流</title>
      <link>//localhost:1313/cn/2024/12/11/yan/</link>
      <pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2024/12/11/yan/</guid>
      <description>
        <![CDATA[
        <p>昨天跟在上海读博士的炎哥交流一番，重获激情人生。</p>
<p>交流的起因是自己心情有点丧，一来是否定自己在学业（讲课）上的付出，而没能兼顾得了毕业的事情，与大家一样，认为这是本末倒置。二来确实是刚从讲课的战场转到论文的战场，能量有点耗尽了，也确实碰壁了，无所适从，无从下手。</p>
<p>但是，与炎哥交流之后，什么都不怕了，什么也不管了，不怕延毕，也不否定自己在学业上的付出了。炎哥说，能认真做好一件无意义的事情很棒呀，无需自责，他最近就常常在做一些无关学习（毕业）的事情，经历很多有意思的事情，比如听话剧，听弦乐，参加党史活动等等。最重要的是自己是自己人生的创造者，去创造一些自己认为有成就感、有意思的事情，做了一些无意义的事情，无关学习的事情怎么样了呢？认真做事，自己开心了就好了，怎么会是错呢？</p>
<p>其实，与炎哥交流，他没有回答心情丧应该怎么做的问题，反而是跳脱出来，聊的是人生，聊得是宏观架构，自然就会减少一些不必要的内耗。难，谁不难。快乐也是一天，难过也是一天，为什么要难过。你是自己人生的主人，你在创造自己的历史，你想怎么创造都可以，但是走到岁月的尽头，再回首，你希望自己是怎么创造的呢？</p>
<p>与炎哥交流完，学习到炎哥身上，即使累，但仍打鸡血地生活态度。我从丧丧的情绪里走出来，最后开心地跟炎哥讲，这一通电话，保三年。</p>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>读博日常</title>
      <link>//localhost:1313/cn/2024/12/06/today/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2024/12/06/today/</guid>
      <description>
        <![CDATA[
        <p>从开学至现在，已然四个月过去了，今天才有感觉，能好好写个日记，记录自己的生活。</p>
<p>首先，心情会很好的原因是，终于要结束了本学期所有要上台的展示。每一次上台，都跟打仗一样，得做好万无一失的准备。每一次可能长达40小时的准备，从自己理清证明，到设计板书的呈现，在脑海不断地多预演几遍，直到真的呈现完，才算是结束一桩事情。这件事才不会像是电脑后台一项一直占着很大内容的任务消耗着大脑的资源，处理不了别的事情。但，仍然不能尽善尽美。</p>
<p>昨天星期四，下午高维课，童老师是本次研讨课的上课老师。虽然，下课的时候，童老师说看得出我有认真准备，但是在讲授过程中，我在证明定理时要用到一个引理，结果问题就出在这个引理上，因为我的重心在于找到这个引理作为搭桥过渡，而没有去证明，想当然地去用，得出自己的结论就好了。万万没想到，大家在这个引理上做了很多讨论，但是我没有去证明也不敢乱讲。我有点被问得呆住了。最难受的是，大家觉得这个东西是我讲错了，只因为我没有证明这个引理，认为我乱用了这个引理。我表示很难受。因为我调查了大量的资料，我是看到严肃的论文都是这样推断的，才敢去呈现我的结果。结果，不仅没有得到一个好的呈现，还让大家觉得这个定理很乱，与我本意想给出一个简洁对称的结果相违背。并且，我更想说，我证的是对的，不能因为我没有说清楚而说它是错的！！这个不能忍。</p>
<p>为了说明我是对的。从昨天下课后，我的大脑就一直在琢磨这个事情，甚至到了凌晨还没困意，就因为想要说明自己没有错。今天一起来，饭也没心思吃，找了很多材料去佐证自己是对的，下午本来想着证据充分了，干点别的，结果还是不自觉回到这个引理的证明上，继续找资料，找更加直接的证明，证明我是对的。没想到，我找到了，在<a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf">高维概率论</a>这本书上163页给出的Gaussian interpolation方法证明过程与我在黑板呈现的是一致的。我找到铁的证据了，恨不得下周四赶紧到来，让我给这个引理平反昭雪！！</p>
<p>所以，心情好的第二原因就是，最后一次上台展示需要的证据有了，讲完就好了！就彻底这个学期课程任务算是解放了。</p>
<p>尽管知道，事情是一波接一波，没有完的时候，但结束一件是一件吧～</p>
<p>通过这件事情，我再次发现自己犟起来真的很犟啊！🤦明明这个定理自己以后也不需要，也没有人在乎这个定理能不能证，过去了也不会有人提了，我还继续拿出来硬要讲清楚它。真犟。</p>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>周二汇报</title>
      <link>//localhost:1313/cn/2024/11/25/storey/</link>
      <pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2024/11/25/storey/</guid>
      <description>
        <![CDATA[
        <blockquote>
<p>汇报人：唐洁
内容：两个方面。第一个，初代Storey方法；第二个，变种Storey方法。围绕提出动机，如何解释，实际效果展开讲述。</p>
</blockquote>
<h1 id="主题-storey方法">主题：Storey方法</h1>
<h2 id="研究问题-多重假设检验">研究问题：多重假设检验</h2>
<p>单个假设检验的思想方法是在控制第一类错误的基础上控制第二类错误，保证两类错误的概率分别能在<code>$\alpha$</code>和<code>$\beta$</code>内。</p>
<p>与单个假设检验相对的概念是多重假设检验。</p>
<p>与单个假设检验一样，多重假设检验可以看作一个检验族，它的重要任务就是控制第一类错误概率的前提下提高检验的功效，尽可能多的发现显著性检验。</p>
<p>多重假设检验的首要问题是怎样定义“错误”，即，错误测度。</p>
<h2 id="研究背景">研究背景：</h2>
<p>随着科学技术的不断发展，当前生物学、医学、金融等发展背景下，高维数据不断涌现，由此导致的传统统计分析方法不再适用。多重假设检验作为分析高维数据的一个重要基础，得到了越来越多的关注。</p>
<h2 id="研究现状">研究现状：</h2>
<ul>
<li>family-wise error rate (FWER) - Shaffer(<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=S+HAFFER+%2C+J.+%281995%29.+Multiple+hypothesis+testing%3A+A+review.+Annual+Review+of+Psychology+46+561%E2%80%93584.&amp;btnG=">1995</a>)</li>
<li>false discovery rate (FDR) - BH(<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=B+ENJAMINI+%2C+Y.+and+H+OCHBERG+%2C+Y.+%281995%29.+Controlling+the+false+discovery+rate%3A+A+practical+and+powerful+approach+to+multiple+testing.+J.+Roy.+Statist.+Soc.+Ser.+B+57+289%E2%80%93300.&amp;btnG=">1995</a>)</li>
<li>positive false discovery rate (pFDR) - Storey (<a href="https://academic.oup.com/jrsssb/article/64/3/479/7098513">2002</a>, <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-6/The-positive-false-discovery-rate--a-Bayesian-interpretation-and/10.1214/aos/1074290335.full">2003</a>)
<ul>
<li>Difference of Slopes Storey (DOS-Storey) - <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kostic,+A">Anica Kostic</a> and <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Fryzlewicz,+P">Piotr Fryzlewicz</a> (2023)</li>
</ul>
</li>
<li>a model-free FDR-controlling  procedure - Barber and Candes (2015)</li>
<li>e-value - Grunwald et al. (2020)， Shafer (2021)， Vovk and Wang (2021)， Xu et al. (2021)， Ignatiadis et al. (2022)， <strong>Wang and Ramdas (2022)</strong>，Dunn et al. (2023)， Xu and Ramdas (2023)</li>
<li><a href="https://candes.su.domains/teaching/stats300c/index.html">Stats 300C</a> - 李冠巡老师推荐多重假设检验理论介绍</li>
</ul>
<h2 id="符号引入">符号引入：</h2>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:left">落入接受域</th>
<th style="text-align:left">落入拒绝域</th>
<th style="text-align:center">总数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>$H_0$</code></td>
<td style="text-align:left">U</td>
<td style="text-align:left">V：犯第一类错误的总数</td>
<td style="text-align:center"><code>$m_0$</code></td>
</tr>
<tr>
<td style="text-align:center"><code>$H_1$</code></td>
<td style="text-align:left">T：犯第二类错误的总数</td>
<td style="text-align:left">S</td>
<td style="text-align:center"><code>$m_1$</code></td>
</tr>
<tr>
<td style="text-align:center">总数</td>
<td style="text-align:left">W</td>
<td style="text-align:left">R：拒绝原假设的总个数</td>
<td style="text-align:center"><code>$m$</code></td>
</tr>
</tbody>
</table>
<p>其中，<code>$m$</code>已知。<code>$m_0$</code>是基于 <code>$p$</code> 值在不同假设下分布的差异性。U、V、T、S在检验中都是不可观察的随机变量，W、R是可观察的随机变量。</p>
<p>对于多个假设检验的最首要的问题是如何控制错误拒绝原假设的个数V或者犯错比率V/R。</p>
<p>因此，多重假设检验问题就是制定一种合理的检验法则来控制犯第一类错误的概率，并且使得检验功效达到最大。</p>
<p>检验法则根据错误测度不同而不同。</p>
<h2 id="历史方法">历史方法：</h2>
<h3 id="fwer">FWER</h3>
<ul>
<li>
<p><strong>定义</strong>：至少出现一次假阳性事件（本为原假设判拒）<code>$ \Pr(V \geq 1)$</code></p>
</li>
<li>
<p><strong>优缺点</strong>：</p>
<ul>
<li>
<p>优点：总体犯第一类错误控制在<code>$\alpha$</code>内。</p>
</li>
<li>
<p>缺点：<code>$m \to \infty$</code>，犯第一类错误的概率为 <code>$1-(1-\alpha)^m \to 1$</code>，失控</p>
</li>
<li>
<p>缺点：<code>$m \to \infty$</code>，每个假设检验的<code>$ p$</code> 值要 <code>$\leq \alpha/m$</code>，严苛</p>
</li>
</ul>
</li>
<li>
<p><strong>Bonferroni 过程</strong>：</p>
<ul>
<li><code>$m$</code>个假设检验，给定检验水平<code>$\alpha$</code>，</li>
<li>设置截断点<code>$S = \cfrac{\alpha}{m}$</code>；</li>
<li>如果<code>$p_i \le S$</code>，拒绝原假设<code>$H_{0i}$</code></li>
</ul>
</li>
<li>
<p><strong>Step-down 过程</strong>：</p>
<ul>
<li><code>$m$</code>个假设检验，给定检验水平<code>$\alpha$</code></li>
<li><code>$ p$</code> 值排序从小到大</li>
<li>截断点<code>$S_i=\cfrac{\alpha}{m-i+1}$</code></li>
<li>如果<code>$p_{(i)} \le S_i \le \cfrac{\alpha}{m}$</code>，拒绝原假设<code>$H_{0i}$</code></li>
</ul>
</li>
<li>
<p><strong>Step-up 过程</strong>：</p>
<ul>
<li><code>$ p$</code> 值排序从大到小</li>
</ul>
</li>
</ul>
<p>考虑到<code>$m \to \infty$</code>时，犯错不可控，根据实际情况，将检验关心的问题更改为：</p>
<p>尽量识别出差异，能够容忍和允许在R次拒绝中发生少量的错误识别。</p>
<p>换而言之，允许犯错更多一点，错误测度可以再宽松一点。</p>
<p>因此，比起控制 <code>$ \Pr(V \geq 1)$</code>，现在是控制<code>$\frac{V}{R} \leq \alpha$</code> 。</p>
<p><code>$\frac{V}{R} \to 0$</code>，所有拒绝中全部判对，无失误，</p>
<p><code>$\frac{V}{R} \to 1$</code>，所有拒绝中全部判错，全失误。</p>
<p>但 <code>$R = 0$</code> 给定义造成困难，解决方案：（A）<code>$E(\frac{V}{R} | R &gt; 0) \Pr(R &gt; 0)$</code> （B）<code>$E(\frac{V}{R} | R &gt; 0)$</code> （C）<code>$\frac{E(V)}{E(R)}$</code></p>
<h3 id="fdr">FDR</h3>
<ul>
<li>
<p><strong>定义</strong>：在所有拒绝次数中错误发现的期望比例。<code>$PFD = E(\frac{V}{R \bigvee 1}) = E(\frac{V}{R} | R &gt; 0) \Pr(R &gt; 0)$</code></p>
</li>
<li>
<p><strong>优缺点</strong>：</p>
</li>
<li>
<p>优点：<code>$FDR \le \alpha$</code></p>
</li>
<li>
<p>优点：当<code>$m=m_0$</code>时，FWER=FDR；当<code>$m&gt;m_0$</code>时，FWER &gt; FDR；FDR比FWER宽松，检验功效大大提高。</p>
</li>
<li>
<p>缺点：</p>
</li>
<li>
<p><strong><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=B+ENJAMINI+%2C+Y.+and+H+OCHBERG+%2C+Y.+%281995%29.+Controlling+the+false+discovery+rate%3A+A+practical+and+powerful+approach+to+multiple+testing.+J.+Roy.+Statist.+Soc.+Ser.+B+57+289%E2%80%93300.&amp;btnG=">BH</a>过程</strong>：</p>
<ul>
<li><code>$m$</code>个假设检验，给定检验水平<code>$\alpha$</code></li>
<li><code>$p$</code> 值从小到大排序</li>
<li>找到截断点<code>$S_i=\cfrac{i}{m}\alpha$</code></li>
<li>如果<code>$p_{(i)} \le S_i$</code>，拒绝原假设<code>$H_{0i}$</code>。（换而言之，找到<code>$p_{(\hat{k})}$</code>， <code>$\hat{k} = \max \{k: p_{(k)} \le \frac{k}{m} \alpha\}$</code>， 拒绝前 <code>$k$</code> 个原假设）</li>
<li>得到相应的调整后的 <code>$p$</code> 值</li>
</ul>
</li>
</ul>
<p>对截断点的选取不同，方法名称不同，如：Benjamini and Liu (1999)，Benjamin and Yekutieli (2001)，</p>
<p>随着对FDR控制方法的深入研究，发现在假设检验中引入<strong>正确原假设比例的估计</strong><code>$\pi_0 = \cfrac{m_0}{m}$</code><strong>能提高检验的功效</strong>，找到更多的显著变量，同时也能很好地控制第一类错误在一个合理的范围内。于是，很多研究提出对于正确原假设比例的估计方法，如：<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=BENJAMINI+Y%EF%BC%8CHOCHBERG+Y%EF%BC%8EThe+adaptive+control+of+the+false+discovery+rate+in+multiple+hypothesis+testing+with+independent+test+statistics%5BJ%5D+%EF%BC%8E+Journal+of+Educational+Behavior+Statistics%EF%BC%8C2001%EF%BC%8C25%281%29%EF%BC%9A60-83.&amp;btnG=">最低斜率估计法</a>，<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=**STOREY+J+D%EF%BC%8EA+direct+approach+to+false+discovery+rates%5BJ%5D%EF%BC%8EJournal+of+the+Royal+Statistical+Society%EF%BC%8C2002%EF%BC%8C64%283%29%EF%BC%9A477-498%EF%BC%8E&amp;btnG="><code>$\lambda$</code> 估计法</a>，<a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=LANGAAS+M%EF%BC%8CFERKINGSTAD+E%EF%BC%8CLINDQVIST+BH%EF%BC%8EEstimating+the+proportion+of+true+null+hypotheses%EF%BC%8Cwith+application+to+DNA+microarray+data%5BJ%5D%EF%BC%8EJournal+of+the+Royal+Statistical+Society%EF%BC%8C2005%EF%BC%8C67%284%29%EF%BC%9A555-572%EF%BC%8E&amp;btnG=">减密度估计法</a>。</p>
<h2 id="storey方法">Storey方法:</h2>
<ul>
<li>
<p><strong>想法提出</strong> - The positive false discovery rate: A bayesian interpretation and the q-value (Storey, <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-6/The-positive-false-discovery-rate--a-Bayesian-interpretation-and/10.1214/aos/1074290335.full">2003</a>)</p>
</li>
<li>
<p><strong>实现步骤</strong> - A direct approach to false discovery rates (Stoter, <a href="https://academic.oup.com/jrsssb/article/64/3/479/7098513">2002</a>)</p>
</li>
<li>
<p><strong>具体应用</strong> -</p>
</li>
</ul>
<h3 id="pfdr">pFDR</h3>
<ul>
<li>
<p><strong>定义</strong>：阳性错误拒绝率，阳性指的是基于至少拒绝一个原假设的事实。<code>$pFDR = E(\frac{V}{R} | R &gt; 0)$</code>。Storey ( <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-31/issue-6/The-positive-false-discovery-rate--a-Bayesian-interpretation-and/10.1214/aos/1074290335.full">2003</a>)</p>
</li>
<li>
<p><strong>想法</strong>：考虑到<code>$m \to \infty$</code>时，<code>$ \Pr(R &gt; 0) \to 1$</code>，有 <code>$E(\frac{V}{R} | R &gt; 0) \Pr (R &gt; 0) \to E(\frac{V}{R} | R &gt; 0)$</code>。</p>
</li>
<li>
<p><strong>优点</strong>：</p>
<ul>
<li>好解释性，在已知原假设的先验概率下，<code>$ pFDR = \Pr (H=0|T \in \Gamma)$</code>，当拒绝原假设时，该假设为真实原假设的概率。</li>
<li>贝叶斯角度，<code>$pFDR = \Pr (H=0|T \in \Gamma) = \cfrac{\pi_0 \Pr(T \in \Gamma|H=0)}{ \Pr(T \in \Gamma)} = \cfrac{\pi_0  \Pr(T \in \Gamma|H=0)}{\pi_0 \Pr(T \in \Gamma|H=0) + \pi_1 \Pr(T \in \Gamma|H=1)}$</code>，可看出，第一类错误越小，功效函数越高，<code>$pFDR$</code>越小，该表达式为第一类错误<a href="https://jesseyule.github.io/machinelearning/bayesian/content.html">贝叶斯后验概率</a>。</li>
<li>实验上，比起FDR，同样的错误控制率但功效高。</li>
<li>理论上，</li>
</ul>
</li>
<li>
<p><strong>缺点</strong>：</p>
</li>
<li>
<p><strong><code>$pFDR$</code> 与 <code>$FDR$</code> 的估计</strong> - Storey (<a href="https://academic.oup.com/jrsssb/article/64/3/479/7098513">2002</a>）：</p>
<ul>
<li>
<p>将拒绝域换成<code>$\{P \le \gamma\}$</code>，则 <code>$pFDR = \cfrac{\pi_0 \Pr(T \in \Gamma|H=0)}{\Pr(T \in \Gamma)} = \cfrac{\pi_0\Pr(P \le \gamma|H=0)}{ \Pr(P \le \gamma)} = \cfrac{\pi_0 \gamma }{\Pr(P \le \gamma)}$</code> （<a href="https://fengchao.pro/blog/proof-that-p-values-under-the-null-are-uniformly-distributed/">为什么 P 值是均匀分布的？</a>）</p>
</li>
<li>
<p><code>$\hat{m}_0 = \cfrac{\#\{p_i &gt; \lambda\}}{1-\lambda}$</code></p>
</li>
<li>
<p><code>$\hat{\pi}_0 = \cfrac{\hat{m}_0}{m} = \cfrac{ \#\{ p_i &gt; \lambda\} /m }{(1-\lambda)} \triangleq \cfrac{W(\lambda) / m}{(1-\lambda)}$</code></p>
</li>
<li>
<p><code>$\widehat{ \Pr}(P \le \gamma) = \cfrac{ \#\{ p_i \le \gamma\} }{m} = \cfrac{R(\gamma)}{m}$</code></p>
</li>
<li>
<p><code>$\widehat{pFDR}_1 = \cfrac{\hat{\pi}_0 \gamma }{\widehat{ \Pr}(P \le \gamma)}= \cfrac{W(\lambda) \gamma}{(1-\lambda)R(\gamma)}$</code> (大样本估计)，(Storey (<a href="https://academic.oup.com/jrsssb/article/64/3/479/7098513">2002</a>)，Section6证明是个好的渐近估计)</p>
</li>
<li>
<p><code>$\widehat{pFDR}_2 = \cfrac{\hat{ \pi}_0 \gamma }{\widehat{\Pr}(P \le \gamma)\{1-(1-\gamma)^m \}}= \cfrac{W(\lambda) \gamma}{(1-\lambda) \{ R(\gamma) \bigvee 1 \}\{1-(1-\gamma)^m \}}$</code> (小样本估计)</p>
</li>
<li>
<p><code>$\widehat{FDR}_{\lambda}(\gamma) = \cfrac{\hat{\pi}_0 \gamma }{\widehat{ \Pr}(P \le \gamma)}= \cfrac{W(\lambda) \gamma}{(1-\lambda) \{ R(\gamma) \bigvee 1 \}}$</code></p>
</li>
</ul>
</li>
<li>
<p><strong>Storey过程</strong>：</p>
<ul>
<li><code>$m$</code> 个假设检验，给定检验水平<code>$\alpha$</code>，给定<code>$\lambda$</code>，计算 <code>$ p$</code> 值</li>
<li>计算 <code>$\hat{\pi}_0(\lambda) =\cfrac{W(\lambda)}{(1-\lambda)m}$</code> 和 <code>$\widehat{ \Pr}(P \le \gamma) = \cfrac{R(\gamma) \bigvee 1}{m}$</code>，其中 <code>$W(\lambda) = \#\{ p_i &gt; \lambda\} $</code>，<code>$R(\gamma) = \#\{ p_i \le \gamma\} $</code></li>
<li>计算 <code>$\widehat{pFDR}_{\lambda}(\gamma) = \cfrac{\hat{\pi}_0 \gamma }{\widehat{\Pr}(P \le \gamma)\{1-(1-\gamma)^m \}}$</code></li>
<li>B个Bootstrap抽样，<code>$b = 1, \dots, B$</code>，计算 <code>$\widehat{pFDR}_{\lambda}^{*b}(\gamma) $</code></li>
</ul>
</li>
</ul>
<h3 id="借鉴storey思想的论文">借鉴Storey思想的论文：</h3>
<h4 id="1-使用storey提出的-hat-m-0-lambda-作为检验过程的一环">1. 使用Storey提出的<code>$\hat{m}_0(\lambda)$</code>作为检验过程的一环</h4>
<ul>
<li><strong>ALBH过程</strong>：
<ul>
<li><code>$m$</code> 个假设检验，给定检验水平<code>$\alpha$</code>，给定<code>$\lambda$</code></li>
<li><code>$ p$</code> 值从小到大排序</li>
<li>计算 <code>$\hat{m}_0 = \cfrac{\#\{p_i &gt; \lambda\}}{1-\lambda}$</code>（<code>$\hat{m}_0$</code>依赖 <code>$\lambda$</code> 的选取，建议取0.5或者 <code>$ p$</code> 值中位数。）</li>
<li>计算 <code>$\alpha^* = \cfrac{m\alpha}{\hat{m}_0}$</code></li>
<li>调用BH过程，以 <code>$\alpha^* $</code> 代替 <code>$\alpha$</code></li>
</ul>
</li>
</ul>
<h4 id="2-提供storey方法中-lambda-的估计方法">2. 提供Storey方法中<code>$\lambda$</code>的估计方法</h4>
<ul>
<li><strong>DOS-Storey过程</strong>：
<ul>
<li>变点方法估计<code>$\lambda$</code>，<code>$ \lambda = p_{\hat{k}} \Rightarrow \hat{m}_0 = \cfrac{\#\{p_i &gt; p_{\hat{k}}\}}{1-p_{\hat{k}}} \Rightarrow \hat{\pi}_0(p_{\hat{k}}) = \cfrac{\# \{ p_i &gt; p_{\hat{k}} \}/m }{(1-p_{\hat{k}})} = \cfrac{1 - \# \{ p_i \le p_{\hat{k}} \}/m }{(1-p_{\hat{k}})} = \cfrac{1 - \hat{k}/m }{(1-p_{\hat{k}})}$</code></li>
<li><code>$ \hat{\pi}_0(p_{\hat{k}}) =\cfrac{1- \hat{F}_n(p_{\hat{k}}) } {1-p_{\hat{k}}} \Rightarrow \hat{\pi}_1(p_{\hat{k}}) =\cfrac{\hat{F}_n(p_{\hat{k}}) - p_{\hat{k}}} {1-p_{\hat{k}}} = \cfrac{ \hat{k}/m  - p_{\hat{k}}}{1-p_{\hat{k}}}$</code></li>
<li><code>$\hat{k}_{\alpha} = \arg \max_{nc_n \le i \le n/2} d_{\alpha}(i) $</code></li>
<li><code>$d_{\alpha}(i) = \cfrac{p_{(2i)} - p_{(i)}}{i^{\alpha}} - \cfrac{p_{(i)}}{i^{\alpha}}$</code>，<code>$\alpha \in (1/2, 1)$</code></li>
</ul>
</li>
<li><strong><a href="https://kns.cnki.net/kns8s/defaultresult/index?crossids=YSTT4HG0%2CLSTPFY1C%2CJUP3MUPD%2CMPMFIG1A%2CWQ0UVIAA%2CBLZOG7CK%2CPWFIRAGL%2CEMRPGLPA%2CNLBO1Z6R%2CNN3FJMUV&amp;korder=SU&amp;kw=A%20Change-Point%20Approach%20to%20Estimating%20the%20Proportion%20of%20False%20Null%20Hypotheses%20in%20Multiple%20Testing">变点方法</a></strong>：
<ul>
<li><strong>定义</strong>：在统计学中，变点指的是在某一位置或时刻，数据或观测值发生显著变化的点。在这个点之前和
之后，数据遵循两个不同的模型或分布，反映了事物的某种特征发生了改变。（<a href="https://kns.cnki.net/kcms2/article/abstract?v=iAN2XHIMbKuxEiPhNkux31bh9rUPt1L-FlfO5YI3NhJDuPFvIjxU1SJAfSJN9RwULPuiR7NwYtnXl-hpipqTMaL3a9gu1GbW0gs6BbC_Sg5u7Y221ebWVnl6eBC5lu0CB_OVwhpHRJntGgVtfdIAFeJSI6xiy6lXlSConMDjcvIzJKLwFKTg0PdM-OBQcEX2gh8dq5yM_PVYgF6ww2Fpfw==&amp;uniplatform=NZKPT">删失回归模型中的变点问题研究</a>）</li>
<li><strong>意义</strong>：对于理解和预测数据的动态变化具有重要意义。</li>
<li><strong>应用</strong>：在<strong>质量控制领域</strong>，在生产过程中，人们往往需要监测生产数据的变化，以便及时发现并解决潜在的问题，在这种情况下，变点就可能表示生产过程中某种因素发生了改变， 比如原材料的更换、设备故障等。通过研究变点，质量控制专家可以更好地了解生产过程变化，并及时采取措施来确保产品质量。在<strong>经济领域</strong>，变点可以用来研究货币政策调整、股市波动等因素变化。在<strong>医学领域</strong>，变点可以帮助研究者更好地理解药物的疗效机制，以及制定更合理的治疗方案。变点模型用于研究气候突变、灾异事件以及地质过程的变化。通过对气候和地质数据的分析，可以识别出数据中的变点，进而了解这些自然现象的变化规律和趋势。</li>
<li><strong>研究方向</strong>：一是估计变点位置，二是对变点存在性进行检验，三是检测变点个数。</li>
</ul>
</li>
</ul>
<h2 id="研究空间">研究空间：</h2>
<ul>
<li>选择更好 <code>$\lambda$</code> 估计 pFDR值</li>
<li>e-value代替p-value</li>
<li>应用到各种高维数据集上</li>
</ul>
<h2 id="总结">总结：</h2>
<ol>
<li>Storey方法：
<ul>
<li>动机：更合理的PDF度量</li>
<li>贝叶斯解释：给定假设性下，是后验贝叶斯概率</li>
<li>如何选择拒绝域：人为给定？</li>
</ul>
</li>
<li>DOS-Storey方法：
<ul>
<li>动机：给出更好的 <code>$\hat{\pi}_0(\lambda)$</code> 估计</li>
<li>区别Storey地方：<code>$\lambda$</code> 取最大变点位置的 <code>$p$</code> 值</li>
<li>实际效果：应用场景广，不论稀疏或不稀疏；保持低偏差同时减少方差。</li>
</ul>
</li>
</ol>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>经验过程</title>
      <link>//localhost:1313/cn/2024/11/25/empirical-process/</link>
      <pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2024/11/25/empirical-process/</guid>
      <description>
        <![CDATA[
        <h1 id="经验过程">经验过程</h1>
<h2 id="书籍推荐">书籍推荐</h2>
<ul>
<li>
<p><a href="https://sites.stat.washington.edu/people/jaw/RESEARCH/TALKS/Delft/emp-proc-delft-big.pdf#page=29.10">EMPIRICAL PROCESSES: Theory and Applications.</a></p>
</li>
<li>
<p><a href="https://zh.z-lib.gl/book/535788/b11796/weak-convergence-and-empirical-processes.html">Weak convergence and empirical processes (1996).</a> Aad van der Vaart, Jon Wellner</p>
</li>
<li>
<p><a href="https://zh.z-lib.gl/book/25469742/e11e5f/weak-convergence-and-empirical-processes-with-applications-to-statistics.html">Weak Convergence and Empirical Processes: With Applications to Statistics (2023).</a> A.W. van der Vaart • Jon A. Wellner</p>
</li>
<li>
<p><a href="https://zh.z-lib.gl/book/3690514/f84568/empirical-processes-with-applications-to-statistics.html">Empirical Processes with Applications to Statistics.</a> Galen R. Shorack, Jon A. Wellner</p>
</li>
</ul>
<h2 id="视频推荐">视频推荐</h2>
<ul>
<li><a href="https://www.bilibili.com/video/BV1qP411k7aG?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=813a147d7428303db620774cb1ec7ba8">经验过程开坑</a></li>
</ul>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>积累</title>
      <link>//localhost:1313/cn/2024/10/07/bnuz/</link>
      <pubDate>Mon, 07 Oct 2024 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2024/10/07/bnuz/</guid>
      <description>
        <![CDATA[
        <h3 id="一些博主">一些博主</h3>
<ul>
<li><a href="https://www.cnblogs.com/leftnoteasy">LeftNotEasy</a> ：关注于 机器学习、数据挖掘、并行计算、数学</li>
</ul>
<h3 id="奇异值">奇异值</h3>
<ul>
<li><a href="https://blog.csdn.net/csyifanZhang/article/details/105937638">深度理解矩阵的奇异值，特征值</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/480389473">彻底搞懂矩阵奇异值分解（SVD）</a></li>
</ul>
<h3 id="算子范数">算子范数</h3>
<ul>
<li><a href="https://www.zhihu.com/column/matrix-learning">矩阵理论学习笔记</a></li>
<li><a href="https://blog.csdn.net/weixin_41094315/article/details/112253105">数值分析6 - 向量范数、矩阵范数、算子范数概念</a></li>
</ul>
<h3 id="一些技巧">一些技巧</h3>
<ul>
<li><a href="https://xj.123147.top">科学上网</a></li>
<li><a href="https://www.bilibili.com/read/cv16673703/">zotero一次性下载所有文献</a></li>
</ul>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title>新生自传</title>
      <link>//localhost:1313/cn/2024/09/04/self-intro/</link>
      <pubDate>Wed, 04 Sep 2024 00:00:00 +0000</pubDate>
      <author>Tan Jay</author>
      <guid>//localhost:1313/cn/2024/09/04/self-intro/</guid>
      <description>
        <![CDATA[
        <p>我的名字叫唐洁，1995年生人，来自湖南祁东，以下是我的自传内容分为三个部分，个人成长经历，自我评价和大学生活规划。</p>
<p>个人成长经历：个人姓名及姓名的来历；家庭基本情况及对家庭氛围的感受；从有记忆以来，经历过哪些对自己有重要影响的事情，以及这些事情是如何影响自己的；家人、朋友、老师对你的印象和评价是什么；个人有哪些兴趣爱好，阅读过哪些印象深刻的书，看过哪些记忆深刻的影视作品。</p>
<p>自我评价：个人如何评价自己，认为自己有哪些性格特点，这些特点对自己生活的影响等。</p>
<p>大学生活规划：个人对北师大的印象及入学适应情况；个人对大学学习及生活的规划，希望如何度过自己的大学，对未来的人生目标是什么等。</p>
<h2 id="个人成长经历">个人成长经历</h2>
<p>贯穿整个个人成长过程中，影响最深的人是母亲吧。至今最让我母亲一直津津乐道的事情，就是在两岁的时候写了一手好粉笔字，还能认两百个字。其实，我对自己两岁能认字、写字的事情，毫无印象，如果这是真的，那只能说明是自己母亲很优秀，她下了很大的功夫，认真教我这些事，是她的功劳。</p>
<p>我正儿八经记事是7岁的时候，那时我本该上小学三年级，因为家里发生了很大的变故，母亲伤心地离开了父亲，独自去往广东东莞，湖南老家留下我和妹妹，还有她恨透的男人。之后，父亲带着7岁的我和4岁的妹妹从湖南到了广东寻找母亲。之后整个小学我都在广东念完，我开始对事情有记忆了。对，石碣序伦小学的校长说我太小，给我留了一级，重新从二年级念起。父母到了广东白手起家，刚开始经济压力还是大的，在我三年级的时候还多了一个弟弟，母亲总是早出晚归，一边背着年幼的弟弟，一边摆摊挣钱，小时候不懂那么多，但是无形之中，我能感受到家里气氛很压抑，我喜欢在学校。回忆里的小学校园充满阳光，有游泳池，有广播站，有节目，有音乐课，有美术课，有电脑课，还有借书室呢！我将所有的感受力都用在了学校，别人捣乱，我不乱，别人想家，我不想。就连每周的值日，我都很认真，我会很早到学校，从值日的那栋大楼的最顶层一个台阶一个台阶地清扫，不放过犄角旮旯。放学路上，我是能走多慢走多慢，我回家路上有个滨江公园，沿着滨江而建，沿路有很多柳树，柳条在空中飘呀飘呀，我可以坐在那里看树看水看很久。若问我读小学的时候在学习上花心思没，我不知道，学习中我是快乐的，我花了心思而不自知吧，就这样顺利升入中学。</p>
<p>我很感谢我的小学老师，他们的谆谆教导使我终身受益。小学的我，还算是全面发展的小孩子吧，因为学校的课都会正常开，包括兴趣课，这与长大后的我去农村教书，兴趣课只是在课表上，实际上是自习课，大相径庭，内心挺不是滋味的。美术老师给了我一个自由的创作空间，她说，这节课自由画画，我选了一棵枯树，只有光秃秃的枝干，我连着三周的美术课都在认真画这棵树，记忆里自己画得挺好，原稿要是保存就好了。音乐老师给了我歌唱的信心，老师在课堂上，让学生轮流在他面前唱歌，有些人会被分到左边，有些人分到右边，我被分到左边，全班同学都唱完之后，老师说左边的人周末跟他训练，进入校歌唱团，原来是这么回事，我很开心被肯定了。电脑老师是学生们最喜欢的老师，没有之一，因为每个小孩都对电脑充满好奇，光是拿鼠标在空白的屏幕上点来点去就可以玩很久。老师教我们怎么制作表格，怎么练习打字等等，有时老师会说，自由冲浪，我问同桌，冲浪怎么冲？同桌一脸鄙夷，然后打开了一个浏览器。厉害的小孩很熟练地打开了小游戏，像我没摸过电脑的就会投去羡慕的目光，那时洋气的小孩都会说自己有QQ，并且等级有一个太阳什么什么的，我是个土鳖，我不知道他们在说啥。语文老师是对于我个人印象很深的老师，他可以徒手在黑板上画一个中国地图，然后开始讲近代史，讲中国共产党如何势如破竹打败国民党的，我们特别喜欢听他讲故事。语文老师做事很认真，大纲要求的每一处须背诵地方，每个小朋友都必须到他跟前背诵，不给弄虚作假的机会，学生交的每一本作业，他要求必须字迹工整，一处不工整整本重写，他还设置班帮扶活动，让前进生帮助后进生，在这个阶段我养成了写日记的习惯。最怕数学老师和英语老师，他们通常要求作业做完做对才能放学，没做完的同学只能眼巴巴看着别人一个个飞奔出教室，很不幸我就被两位老师留过堂。</p>
<p>小学的时候，父母看我反应慢慢的，总跟我说，笨鸟先飞，培养我自学的习惯，但主体上，我还是跟着老师的指引，步调轻盈地迈入中学。</p>
<p>初中第一学期仍在广东念书。这是我树立自信的开始。第一个是成绩，小学老师不宣扬成绩排名，中学的考试座位可是按成绩排的。中学很大，光一个年级的考场就有10多个，从1号开始编号，在1号考场的前面还有一个特殊考场，全年级前50名在这里考试，我很还在这个特殊考场考试过。第二个是军训，这真的得夸广东的教育了，我们是专车到黄埔军校封闭式全天候训练。我自己都能感受到自己那种信念，腰背腿都绷得紧紧的，我能感受到肌肤上每一滴汗水的滑落。我们集合的速度必须快，教官说，我要听到万马奔腾的声音。我们回答教官的声音必须洪亮，教官的命令必须服从。教官们半夜也会搞事情，吹声口哨集合，所有人，脑子清楚的，迷糊的，都刷刷往外跑，尘土可以飞扬一丈多高。别的同学我不知道，我是真的很喜欢这种简单直接明了的方式，我喜欢这种吃苦之后踏实的感觉。军训最后一次集合，台上的教官说，请各教官把“黑名单”交上来，下面的小兵崽议论纷纷，生怕进了黑名单，念到我的名字上了台才知道，原来是发奖状呢。这是我最骄傲的奖状，它搬进了我心里，在我生命的长河里熠熠生辉，激励我进取。第三个是班级，初入中学，新的班级，新的同学，新的老师，全新的一切都需要互相了解，班主任在课堂上，让我们互相写自己心中谁最受欢迎，谁成绩最好，谁最古灵精怪等等。我没想到自己成为班上最受欢迎的那个，我很惊讶很害羞很开心。在学校真的收获很多快乐。</p>
<p>初中第二学期因为籍贯原因，我转学到了湖南老家一个村里的中学，鸣鹿中学，这个学校没有广东中学一个操场大，自然风光却是极好的。学校设施简陋，第一台投影仪是我念初三的时候才有的。学校的学生绝大部分属于留守，同学们都很质朴，率真，跟她们一块就是哈哈大笑。在乡村读书的时光很自由，为了考上心仪的高中，我也很努力。我考上高中那一年，学校很高兴，还在进乡的巴士车身上拉上红幅，庆祝今年初中的赫赫成绩，唯一的毕业班，60人，考上7个，是很不错的成绩了。我很敬佩我的老师们，他们说，他们是当年的中专生，在当年考上中专乡里是要放鞭炮的呢，他们17岁站上讲台，一站就是二十年，不是没有机会离开这里，是他们选择没有走，他们说，虽然吃的用的不是很好，但是有的吃有的用很好了，他们自由时间很多可以任意安排。他们在课堂上分享他们的生活，我的班主任王艳老师，工作认真在乡里是出了名的，附近老百姓都很信任她，全要把孩子往她的班级里送。语文老师很漂亮，一身衣服不重样，上起课来，别看是弱女子，没有一个男孩子敢捣乱。历史老师说，他一周可以读一本很厚的小说，起床先看一部好莱坞大片，他说人读点书有用，他钻研历史教材，对历史了如指掌，出去跟人聊天很有料。政治老师很有一套自己的方法，做什么笔记，笔记记在哪里，他都安排得清清楚楚，每个早自习会抽背，大家都怕背不出来，就很大声地背诵。数学老师很幽默，他开学第一堂课，说，“我知道你们都知识都还给我了，我在家里拿根天线在家里收。”边说还边用手比作天线支在脑门边上，特别可爱。有时静静坐在教室里，望向窗外，老师们其乐融融地聚在一起，坐个小马扎，磕嗑瓜子聊聊天，阳光温柔地落在他们身上，我心想，这是我未来理想生活的模样，有喜欢的事业有热爱的生活。</p>
<p>当我还沉溺于初中的自由快乐中，高中一个巴掌把我扇入了痛苦挣扎中。</p>
<p>刚进入高中，我并没有意识到课程的内容以及难度变大，后知后觉使我陷入被动之中，当我奋起急追时，我已落后太远。按照从前的行为风格，我很快与班级同学熟络了，并票选为班长，我将大部分精力用于班级事务，常常优先于自己的事情，我夜里经常熬夜补习。成绩单很快给出了反应，不行，分数上不去，我陷入努力又无效的窘迫之境。直到高考成绩单出来，我觉得那是自己人生的至暗时刻，二本线附近，老妈为了求稳，希望我未来能走向教师岗位，为我选了独立学院的师范专业。我认真复盘了高中阶段的失败，为什么没有达到自己的预期，明明早上很早就到了，中午快速吃个饭又回到教室了，晚饭时间也留在教室里，为什么没有效果呢。还是心态出了问题，做不好出众，自己便想着表现努力，而非真努力，没有把努力用对地方，只知道要自己坐在教室，但不知道坐在教室应该去做点什么，没有长线的合理的布置。</p>
<p>吸取了高中的失败教训，我开始认知的第一次觉醒，宁可去玩，不要假努力，努力不是给别人看的，是给自己看的，要过自己那一关，认准目标，真正全力以赴。</p>
<p>自大学之后，所有考级都是一次通过，因为在考试前我会合理的布置计划，并强有力地执行，不拖欠计划。于是，顺利通过四级六级，计算机二级三级，硕士完成论文6篇，发表4篇，哪怕是边工作边考博，也丝毫不懈怠每一件到手的事情。自2017年大学毕业，到现如今2024年博士入学，七年的光影，一晃而过，回头看有点像梦一样，我从不入流大学，到普通双非学校，再到北京师范大学一流名校，自己也算完成个人的一部逆袭史了。当然，这个中间不是只有自己努力就可以的，也有很多人对我的帮助。我很感谢本科的周勇老师、周志东老师为我推荐了广西师范大学，遇到了把学生当自己小孩教诲的秦永松教授，对我非常多次关键节点的帮助，同门邱涛师兄也非常感谢秦老师的谆谆教诲，对我们师弟师妹也倾注同样的关爱，在我考学的时候给予我许多无私的指点。我还特别感谢谢传龙老师的信任，他愿意接受我的博士报名，表达对我硕士做的一些工作的认可，这一阶段，我还是想一如既往地，全力以赴地，以感恩之心回馈谢导，博士三年做出点真成绩出来报答谢导知遇之恩。</p>
<p>本科毕业后我先工作了三年，认真带了三届学生后，我又重返校园当起了学生，我在硕士期间有了很多课余时间，读了几本广西师大推荐的好书，我都很受启发，比如阅读《人类简史》感知到愚昧是多么可怕；在《哲学家们都在思考些什么》获得心灵的解脱，原来生活本无意义，所谓意义是自定义；阅读朱光潜先生的《审美》了解为何美；品读周国平大师的《哲思录》感受到文学与逻辑的结合之美；秦永松老师推荐的《杨绛传》感受到人生最重要的内心的淡定与波澜不惊；感叹《从一到无穷大》作者的文笔，将微小世界与宏观世界讲述得那么引人入胜，让我深刻地明白数理化生原来是一家。优秀的书籍还有很多，我看的还很少，希望自己日后保持读书的习惯，还能加强写作习惯的培养，将自己感悟记录下来。</p>
<p>回顾自己的成长历程，一直都没有离开过学校，不论是作为学生还是作为中学教师，我很喜欢学校的简单纯粹的氛围，我还想在这个世界里徜徉，吸收养分，继续生长。</p>
<h2 id="自我评价">自我评价</h2>
<p>经历这么多些年的成长，对自己也有所察觉，我认为自己的性格特点一就是能吃苦，愿意吃苦，做事不偷懒，尽量做到自己的极致，这使我像蜗牛一样，虽然慢，但一直能有进步。我的性格特点二就是比较随和，在生活里会自由一些，与他人相处会考虑对方感受多一些，这让我感受到与人为乐的快乐。简而言之，我为人随和，做事认真。</p>
<h2 id="大学规划">大学规划</h2>
<p>能到北京师范大学念书，感觉很幸运很幸福，有很有实力的谢导领路，有很优秀的室友作伴，有很优美的景色环绕，我深感荣幸，我跟自己讲，一定要倍加珍惜。我的大学学习规划是，每天六点半起床，七点半到工位学习，下午两点继续回到工位，晚上运动之后七点半再次回到工位，保证一天至少九个小时的学习时长，十点回寝洗漱，十一入睡。希望自己经过博一第一年的探索，能顺利找到新论文方向，可以小发一篇论文，博二能产出高质量论文，博三能顺利毕业，整个三年能配合导师做一些有意义的项目。未来就业希望可以当一名高校教师。</p>

        
        ]]>
      </description>
    </item>
    
    
    
    <item>
      <title></title>
      <link>//localhost:1313/2025-00-00-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//localhost:1313/2025-00-00-/</guid>
      <description>
        <![CDATA[
        
        
        ]]>
      </description>
    </item>
    
    
  </channel>
</rss>
